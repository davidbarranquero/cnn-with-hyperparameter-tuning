{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a6db2cd",
   "metadata": {},
   "source": [
    "## David Barranquero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f30e2fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-25 19:31:06.382847: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb57c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba00a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ee764",
   "metadata": {},
   "source": [
    "Setting a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ceaab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 54321\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90e096",
   "metadata": {},
   "source": [
    "We're going to build a convolutional neural network for image classification. The dataset we will use is the MNIST Fashion Dataset, which contains 60,000 training images and 10,000 test images, all 28x28 and in greyscale. The dataset contains an equal amount of images of 10 different clothing articles. We begin first by reading in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfba2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e8379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \n",
    "               \"Trouser\", \n",
    "               \"Pullover\", \n",
    "               \"Dress\", \n",
    "               \"Coat\", \n",
    "               \"Sandal\", \n",
    "               \"Shirt\", \n",
    "               \"Sneaker\", \n",
    "               \"Bag\", \n",
    "               \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d163b6e",
   "metadata": {},
   "source": [
    "Next, we visualise our dataset, viewing the first entry in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8493d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ankle boot\n"
     ]
    }
   ],
   "source": [
    "print(class_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba0a0e",
   "metadata": {},
   "source": [
    "As well as visualising the image itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25109b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff1e770f4f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4d2b9",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a258c",
   "metadata": {},
   "source": [
    "Firstly, we must perform pre-processing on the dataset. We are going to employ Min-Max normalisation on the dataset to place all values between 0 and 1. By normalising, the network will achieve a faster convergence to the local minimum. Fortunately, because of the 256 pixel values ranging from 0 to 255, the normalisation effect is achieved by simply dividing the dataset by 255. To do so, we first convert the dataset type to float, and then perform the normalisation. Furthermore, as general practice for when we deal with colour images and have three RGB colour channels, we reshape the dataset to include this information. We define a function for this preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d8cb2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_data(dataset):\n",
    "    dataset = dataset.reshape((dataset.shape[0], 28, 28, 1))\n",
    "    dataset = dataset.astype('float32')\n",
    "    dataset /= 255\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b00a8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalise_data(X_train)\n",
    "X_test = normalise_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5b543",
   "metadata": {},
   "source": [
    "Furthermore, we define a validation set using the first 10000 images of the training dataset. Ideally, when creating a validation set, it is wise to perform stratified sampling on your dataset. When there are large class imbalances, stratified sampling lowers the sampling variance of dataset split, which translates to better performance for the image classification model. However, if we look at the unique counts of the first 10000 images, we can see that there is a fairly uniform distribution of roughly a thousand per image, so the advantage of stratified sampling is minimal over simple random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada10d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([ 942, 1027, 1016, 1019,  974,  989, 1021, 1022,  990, 1000]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train[:10000], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0000eb1",
   "metadata": {},
   "source": [
    "For this reason, we will use the first 10000 images for our validation set, and the remaining 50000 for our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bfe256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train[:10000], X_train[10000:]\n",
    "y_valid, y_train = y_train[:10000], y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5bd634",
   "metadata": {},
   "source": [
    "Finally, we confirm the set sizes and shapes of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58fcf2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28, 1)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba49bd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1073c466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8023b0a",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208414d",
   "metadata": {},
   "source": [
    "For image classification tasks, convolutional neural networks consistently provide the best performance.\n",
    "\n",
    "Our model will consist of a convolutional layer with 32 filters and the standard 3x3 kernel size, with a stride of 1. We will use a layer of padding to ensure that the output is of the same shape and information is not lost. Finally, for our activation function we will use ReLU. ReLU is defined as ReLU(a) = max(0,a). It leads to faster convergence by promoting sparsity in the model, as well as avoiding the vanishing/exploding gradients issue of other activation functions. \n",
    "\n",
    "Next, we employ a pooling layer using max pooling, again using the standard 2x2 pool size with a step size of 2. Pooling layers are effective at reducing the dimensionality of the dataset while retaining the most important information.\n",
    "\n",
    "This convolution and pooling is then repeated again, with 64 kernels, to capture more complex shapes and images.\n",
    "\n",
    "Finally, we flatten the dataset in order to use the dense layers of standard feed-forward neural networks for the classification. We will use two fully connected layers with 100 neurons each, and the ReLU activation. Each will have a dropout layer with a rate of 25% to create a more robust classifier. Finally, we add an output layer, with the softmax activation, as this is a multi-class classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e2f6143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-25 19:31:17.487533: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3,3), strides=1, padding=\"same\", activation=\"relu\", input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, kernel_size=(3,3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bb7505",
   "metadata": {},
   "source": [
    "Our loss function for this model will be sparse categorical crossentropy, as the target variable contains only the sparse label vector position. We use stochastic gradient descent as our optimiser, which aids in speeding up the convergence to the local minima by taking faster, imprecise steps, rather than slow, precise steps of gradient descent. Finally, we use accuracy as our performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "459eea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f843930",
   "metadata": {},
   "source": [
    "We can display a summary of our model as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e4e206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               313700    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 343,626\n",
      "Trainable params: 343,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f36dea",
   "metadata": {},
   "source": [
    "Next, we fit the model and assess it's performance on the validation set. We will perform 10 sweeps over the entire dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42257441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 1.0758 - accuracy: 0.6052 - val_loss: 0.6264 - val_accuracy: 0.7586\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.6423 - accuracy: 0.7630 - val_loss: 0.5286 - val_accuracy: 0.7959\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.5554 - accuracy: 0.7950 - val_loss: 0.4618 - val_accuracy: 0.8269\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.4976 - accuracy: 0.8183 - val_loss: 0.4274 - val_accuracy: 0.8376\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 45s 28ms/step - loss: 0.4569 - accuracy: 0.8346 - val_loss: 0.3817 - val_accuracy: 0.8627\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.4324 - accuracy: 0.8439 - val_loss: 0.3986 - val_accuracy: 0.8494\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.4089 - accuracy: 0.8515 - val_loss: 0.3556 - val_accuracy: 0.8701\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 45s 28ms/step - loss: 0.3895 - accuracy: 0.8586 - val_loss: 0.3335 - val_accuracy: 0.8780\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.3766 - accuracy: 0.8641 - val_loss: 0.3131 - val_accuracy: 0.8850\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.3637 - accuracy: 0.8683 - val_loss: 0.3127 - val_accuracy: 0.8867\n"
     ]
    }
   ],
   "source": [
    "model_fit = model.fit(X_train, y_train, epochs=10, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6358f",
   "metadata": {},
   "source": [
    "And plot the performance of this model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4cfdd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(model_fit.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d8361",
   "metadata": {},
   "source": [
    "From the plot above, we can see that the training and validation accuracy has consistently increased and the training and validation loss have consistently decreased, which indicates that our model is learning correctly. Furthermore, the closeness of the training and validation curves indicate that our model is not overfitting, but generalising well to unseen data. Finally, we assess the performance of our model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c59797ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 8ms/step - loss: 0.3370 - accuracy: 0.8764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3370104134082794, 0.8763999938964844]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83016f7",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ab159",
   "metadata": {},
   "source": [
    "Having built our base model, we are now going to fine-tune our hyperparameters. For this we will define a function to build the model, and then use a scikit-learn wrapper so that we can use GridSearchCV for our grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81110e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_neurons, opt_type):\n",
    "    \n",
    "    print('n_neurons: {0}, opt_type: {1}'.format(n_neurons, opt_type))\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3,3), strides=1, padding=\"same\", activation=\"relu\", input_shape=(28,28,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt_type, metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4aa605c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_classifier = KerasClassifier(model=build_model, n_neurons=100, opt_type=\"sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93925c0a",
   "metadata": {},
   "source": [
    "While there are numerous hyperparameters we could optimise, for the sake of computational time and complexity, we are only going to optimise two, the number of neurons in the dense layer, and the optimiser used for the model. Adaptive Momentum Estimation (Adam) is another popular optimiser and is generally considered one of the best performing optimisers for deep learning. We will compare it's performance to SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83989e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"n_neurons\": [100, 200],\n",
    "    \"opt_type\": [\"sgd\", \"adam\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e600ec66",
   "metadata": {},
   "source": [
    "For 4 potential hyperparameters, using 5-fold cross validaiton, 10 epochs and roughly 45 seconds training per epoch, the grid search should take about 2.5 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01d487f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_neurons: 100, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 1.1857 - accuracy: 0.5619 - val_loss: 0.6675 - val_accuracy: 0.7456\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 0.7041 - accuracy: 0.7343 - val_loss: 0.5575 - val_accuracy: 0.7847\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 0.5996 - accuracy: 0.7778 - val_loss: 0.5016 - val_accuracy: 0.8078\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.5422 - accuracy: 0.7987 - val_loss: 0.4446 - val_accuracy: 0.8400\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.5003 - accuracy: 0.8168 - val_loss: 0.4214 - val_accuracy: 0.8453\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.4652 - accuracy: 0.8315 - val_loss: 0.3945 - val_accuracy: 0.8558\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.4420 - accuracy: 0.8399 - val_loss: 0.3715 - val_accuracy: 0.8640\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.4214 - accuracy: 0.8483 - val_loss: 0.3666 - val_accuracy: 0.8653\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.4035 - accuracy: 0.8558 - val_loss: 0.3465 - val_accuracy: 0.8744\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.3902 - accuracy: 0.8597 - val_loss: 0.3340 - val_accuracy: 0.8793\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "n_neurons: 100, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 1.2199 - accuracy: 0.5514 - val_loss: 0.6782 - val_accuracy: 0.7519\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7066 - accuracy: 0.7366 - val_loss: 0.5589 - val_accuracy: 0.7905\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6064 - accuracy: 0.7743 - val_loss: 0.4800 - val_accuracy: 0.8228\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.5419 - accuracy: 0.8002 - val_loss: 0.4426 - val_accuracy: 0.8341\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.5018 - accuracy: 0.8154 - val_loss: 0.4338 - val_accuracy: 0.8341\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.4734 - accuracy: 0.8273 - val_loss: 0.3951 - val_accuracy: 0.8582\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.4470 - accuracy: 0.8383 - val_loss: 0.3831 - val_accuracy: 0.8605\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.4274 - accuracy: 0.8454 - val_loss: 0.3674 - val_accuracy: 0.8654\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.4101 - accuracy: 0.8523 - val_loss: 0.3543 - val_accuracy: 0.8712\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.3932 - accuracy: 0.8574 - val_loss: 0.3374 - val_accuracy: 0.8781\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "n_neurons: 100, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 1.1401 - accuracy: 0.5806 - val_loss: 0.7204 - val_accuracy: 0.7289\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7022 - accuracy: 0.7369 - val_loss: 0.5592 - val_accuracy: 0.7921\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6057 - accuracy: 0.7746 - val_loss: 0.4978 - val_accuracy: 0.8179\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.5466 - accuracy: 0.7989 - val_loss: 0.4610 - val_accuracy: 0.8255\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.5087 - accuracy: 0.8163 - val_loss: 0.4331 - val_accuracy: 0.8402\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4788 - accuracy: 0.8257 - val_loss: 0.4031 - val_accuracy: 0.8530\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4525 - accuracy: 0.8367 - val_loss: 0.3948 - val_accuracy: 0.8548\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4317 - accuracy: 0.8440 - val_loss: 0.3704 - val_accuracy: 0.8647\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4164 - accuracy: 0.8502 - val_loss: 0.3600 - val_accuracy: 0.8696\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.3999 - accuracy: 0.8529 - val_loss: 0.3427 - val_accuracy: 0.8754\n",
      "313/313 [==============================] - 3s 9ms/step\n",
      "n_neurons: 100, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 1.1960 - accuracy: 0.5541 - val_loss: 0.6847 - val_accuracy: 0.7493\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7024 - accuracy: 0.7400 - val_loss: 0.5425 - val_accuracy: 0.7981\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.5977 - accuracy: 0.7804 - val_loss: 0.4875 - val_accuracy: 0.8241\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.5392 - accuracy: 0.8047 - val_loss: 0.4519 - val_accuracy: 0.8297\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4977 - accuracy: 0.8199 - val_loss: 0.4167 - val_accuracy: 0.8440\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.4719 - accuracy: 0.8291 - val_loss: 0.3972 - val_accuracy: 0.8565\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4450 - accuracy: 0.8424 - val_loss: 0.3830 - val_accuracy: 0.8577\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4304 - accuracy: 0.8442 - val_loss: 0.3593 - val_accuracy: 0.8684\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.4134 - accuracy: 0.8487 - val_loss: 0.3537 - val_accuracy: 0.8713\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4010 - accuracy: 0.8577 - val_loss: 0.3426 - val_accuracy: 0.8734\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "n_neurons: 100, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 1.1861 - accuracy: 0.5634 - val_loss: 0.6641 - val_accuracy: 0.7574\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6903 - accuracy: 0.7424 - val_loss: 0.5299 - val_accuracy: 0.8030\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.5892 - accuracy: 0.7809 - val_loss: 0.4815 - val_accuracy: 0.8267\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.5335 - accuracy: 0.8047 - val_loss: 0.4429 - val_accuracy: 0.8378\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.4910 - accuracy: 0.8199 - val_loss: 0.4459 - val_accuracy: 0.8324\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4582 - accuracy: 0.8337 - val_loss: 0.3983 - val_accuracy: 0.8579\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4390 - accuracy: 0.8414 - val_loss: 0.3820 - val_accuracy: 0.8624\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4208 - accuracy: 0.8475 - val_loss: 0.3756 - val_accuracy: 0.8599\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4053 - accuracy: 0.8540 - val_loss: 0.3535 - val_accuracy: 0.8703\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.3910 - accuracy: 0.8605 - val_loss: 0.3417 - val_accuracy: 0.8755\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "n_neurons: 100, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.5692 - accuracy: 0.7941 - val_loss: 0.3439 - val_accuracy: 0.8758\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.3599 - accuracy: 0.8718 - val_loss: 0.2880 - val_accuracy: 0.8933\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.3035 - accuracy: 0.8909 - val_loss: 0.2654 - val_accuracy: 0.9051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.2733 - accuracy: 0.9005 - val_loss: 0.2496 - val_accuracy: 0.9072\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2416 - accuracy: 0.9117 - val_loss: 0.2336 - val_accuracy: 0.9142\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2226 - accuracy: 0.9199 - val_loss: 0.2363 - val_accuracy: 0.9173\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.2038 - accuracy: 0.9260 - val_loss: 0.2413 - val_accuracy: 0.9133\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.1885 - accuracy: 0.9327 - val_loss: 0.2298 - val_accuracy: 0.9191\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.1734 - accuracy: 0.9359 - val_loss: 0.2439 - val_accuracy: 0.9173\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.1622 - accuracy: 0.9390 - val_loss: 0.2460 - val_accuracy: 0.9172\n",
      "313/313 [==============================] - 2s 8ms/step\n",
      "n_neurons: 100, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.5805 - accuracy: 0.7870 - val_loss: 0.3426 - val_accuracy: 0.8762\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.3628 - accuracy: 0.8710 - val_loss: 0.2963 - val_accuracy: 0.8913\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.3072 - accuracy: 0.8899 - val_loss: 0.2655 - val_accuracy: 0.9044\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2767 - accuracy: 0.8993 - val_loss: 0.2501 - val_accuracy: 0.9086\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.2457 - accuracy: 0.9105 - val_loss: 0.2582 - val_accuracy: 0.9076\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2245 - accuracy: 0.9183 - val_loss: 0.2443 - val_accuracy: 0.9129\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.2046 - accuracy: 0.9243 - val_loss: 0.2397 - val_accuracy: 0.9123\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.1886 - accuracy: 0.9303 - val_loss: 0.2179 - val_accuracy: 0.9231\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.1749 - accuracy: 0.9338 - val_loss: 0.2281 - val_accuracy: 0.9201\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.1626 - accuracy: 0.9395 - val_loss: 0.2258 - val_accuracy: 0.9222\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "n_neurons: 100, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 40s 31ms/step - loss: 0.5865 - accuracy: 0.7858 - val_loss: 0.3397 - val_accuracy: 0.8749\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.3634 - accuracy: 0.8706 - val_loss: 0.2923 - val_accuracy: 0.8954\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.3073 - accuracy: 0.8893 - val_loss: 0.2670 - val_accuracy: 0.9038\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.2717 - accuracy: 0.9008 - val_loss: 0.2553 - val_accuracy: 0.9108\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.2421 - accuracy: 0.9118 - val_loss: 0.2340 - val_accuracy: 0.9172\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2235 - accuracy: 0.9200 - val_loss: 0.2353 - val_accuracy: 0.9157\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2013 - accuracy: 0.9255 - val_loss: 0.2266 - val_accuracy: 0.9185\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.1875 - accuracy: 0.9302 - val_loss: 0.2214 - val_accuracy: 0.9215\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.1719 - accuracy: 0.9370 - val_loss: 0.2323 - val_accuracy: 0.9183\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.1572 - accuracy: 0.9430 - val_loss: 0.2281 - val_accuracy: 0.9233\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "n_neurons: 100, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.5889 - accuracy: 0.7892 - val_loss: 0.3510 - val_accuracy: 0.8751\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.3637 - accuracy: 0.8714 - val_loss: 0.3044 - val_accuracy: 0.8852\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.3067 - accuracy: 0.8911 - val_loss: 0.2586 - val_accuracy: 0.9067\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2717 - accuracy: 0.9021 - val_loss: 0.2483 - val_accuracy: 0.9103\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2484 - accuracy: 0.9114 - val_loss: 0.2379 - val_accuracy: 0.9165\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.2247 - accuracy: 0.9179 - val_loss: 0.2361 - val_accuracy: 0.9100\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2019 - accuracy: 0.9264 - val_loss: 0.2385 - val_accuracy: 0.9159\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.1894 - accuracy: 0.9301 - val_loss: 0.2264 - val_accuracy: 0.9202\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.1736 - accuracy: 0.9360 - val_loss: 0.2442 - val_accuracy: 0.9155\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.1562 - accuracy: 0.9427 - val_loss: 0.2281 - val_accuracy: 0.9229\n",
      "313/313 [==============================] - 2s 8ms/step\n",
      "n_neurons: 100, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.5610 - accuracy: 0.7960 - val_loss: 0.3405 - val_accuracy: 0.8777\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.3478 - accuracy: 0.8759 - val_loss: 0.2817 - val_accuracy: 0.8943\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2937 - accuracy: 0.8947 - val_loss: 0.2492 - val_accuracy: 0.9106\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.2619 - accuracy: 0.9061 - val_loss: 0.2351 - val_accuracy: 0.9150\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2351 - accuracy: 0.9129 - val_loss: 0.2449 - val_accuracy: 0.9098\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.2132 - accuracy: 0.9216 - val_loss: 0.2223 - val_accuracy: 0.9190\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.1939 - accuracy: 0.9289 - val_loss: 0.2354 - val_accuracy: 0.9167\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.1754 - accuracy: 0.9359 - val_loss: 0.2368 - val_accuracy: 0.9159\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.1635 - accuracy: 0.9395 - val_loss: 0.2338 - val_accuracy: 0.9224\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.1506 - accuracy: 0.9444 - val_loss: 0.2345 - val_accuracy: 0.9213\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "n_neurons: 200, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 1.0565 - accuracy: 0.6131 - val_loss: 0.6115 - val_accuracy: 0.7734\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.6318 - accuracy: 0.7666 - val_loss: 0.5320 - val_accuracy: 0.7955\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.5426 - accuracy: 0.7998 - val_loss: 0.4529 - val_accuracy: 0.8288\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.4855 - accuracy: 0.8214 - val_loss: 0.4162 - val_accuracy: 0.8463\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.4455 - accuracy: 0.8387 - val_loss: 0.3883 - val_accuracy: 0.8583\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.4204 - accuracy: 0.8468 - val_loss: 0.3691 - val_accuracy: 0.8649\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.4024 - accuracy: 0.8541 - val_loss: 0.3577 - val_accuracy: 0.8702\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.3830 - accuracy: 0.8600 - val_loss: 0.3438 - val_accuracy: 0.8728\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 41s 32ms/step - loss: 0.3675 - accuracy: 0.8674 - val_loss: 0.3343 - val_accuracy: 0.8761\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.3557 - accuracy: 0.8717 - val_loss: 0.3141 - val_accuracy: 0.8873\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "n_neurons: 200, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 1.0141 - accuracy: 0.6196 - val_loss: 0.6641 - val_accuracy: 0.7523\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6544 - accuracy: 0.7566 - val_loss: 0.5655 - val_accuracy: 0.7810\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.5581 - accuracy: 0.7919 - val_loss: 0.4588 - val_accuracy: 0.8278\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.5029 - accuracy: 0.8134 - val_loss: 0.4209 - val_accuracy: 0.8452\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4623 - accuracy: 0.8301 - val_loss: 0.4115 - val_accuracy: 0.8462\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4349 - accuracy: 0.8417 - val_loss: 0.3831 - val_accuracy: 0.8593\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4133 - accuracy: 0.8503 - val_loss: 0.3673 - val_accuracy: 0.8673\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 0.3937 - accuracy: 0.8553 - val_loss: 0.3407 - val_accuracy: 0.8759\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 0.3778 - accuracy: 0.8627 - val_loss: 0.3403 - val_accuracy: 0.8750\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 0.3639 - accuracy: 0.8683 - val_loss: 0.3229 - val_accuracy: 0.8837\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "n_neurons: 200, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 41s 32ms/step - loss: 1.0840 - accuracy: 0.5999 - val_loss: 0.6632 - val_accuracy: 0.7622\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.6545 - accuracy: 0.7543 - val_loss: 0.5423 - val_accuracy: 0.7964\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.5651 - accuracy: 0.7895 - val_loss: 0.4714 - val_accuracy: 0.8272\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.5102 - accuracy: 0.8126 - val_loss: 0.4276 - val_accuracy: 0.8406\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.4659 - accuracy: 0.8294 - val_loss: 0.4027 - val_accuracy: 0.8515\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.4344 - accuracy: 0.8409 - val_loss: 0.3764 - val_accuracy: 0.8629\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.4151 - accuracy: 0.8497 - val_loss: 0.3724 - val_accuracy: 0.8633\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.3949 - accuracy: 0.8572 - val_loss: 0.3425 - val_accuracy: 0.8758\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.3775 - accuracy: 0.8630 - val_loss: 0.3365 - val_accuracy: 0.8780\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.3635 - accuracy: 0.8670 - val_loss: 0.3196 - val_accuracy: 0.8834\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "n_neurons: 200, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 1.0676 - accuracy: 0.6082 - val_loss: 0.6183 - val_accuracy: 0.7742\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.6300 - accuracy: 0.7656 - val_loss: 0.5025 - val_accuracy: 0.8116\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 41s 32ms/step - loss: 0.5332 - accuracy: 0.8024 - val_loss: 0.4390 - val_accuracy: 0.8397\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.4804 - accuracy: 0.8240 - val_loss: 0.4175 - val_accuracy: 0.8435\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.4427 - accuracy: 0.8401 - val_loss: 0.3923 - val_accuracy: 0.8566\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.4212 - accuracy: 0.8483 - val_loss: 0.3597 - val_accuracy: 0.8714\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.3983 - accuracy: 0.8559 - val_loss: 0.3634 - val_accuracy: 0.8651\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.3800 - accuracy: 0.8626 - val_loss: 0.3372 - val_accuracy: 0.8763\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.3702 - accuracy: 0.8660 - val_loss: 0.3353 - val_accuracy: 0.8743\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.3547 - accuracy: 0.8699 - val_loss: 0.3259 - val_accuracy: 0.8781\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "n_neurons: 200, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 1.0359 - accuracy: 0.6186 - val_loss: 0.6188 - val_accuracy: 0.7753\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.6338 - accuracy: 0.7629 - val_loss: 0.5179 - val_accuracy: 0.8050\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.5431 - accuracy: 0.7987 - val_loss: 0.4614 - val_accuracy: 0.8334\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.4932 - accuracy: 0.8169 - val_loss: 0.4297 - val_accuracy: 0.8377\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.4544 - accuracy: 0.8352 - val_loss: 0.4015 - val_accuracy: 0.8513\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.4299 - accuracy: 0.8467 - val_loss: 0.3763 - val_accuracy: 0.8643\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.4047 - accuracy: 0.8537 - val_loss: 0.3756 - val_accuracy: 0.8588\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.3895 - accuracy: 0.8611 - val_loss: 0.3482 - val_accuracy: 0.8728\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.3750 - accuracy: 0.8627 - val_loss: 0.3249 - val_accuracy: 0.8839\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 41s 32ms/step - loss: 0.3596 - accuracy: 0.8694 - val_loss: 0.3208 - val_accuracy: 0.8818\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.4992 - accuracy: 0.8206 - val_loss: 0.3177 - val_accuracy: 0.8846\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.3114 - accuracy: 0.8857 - val_loss: 0.2599 - val_accuracy: 0.9063\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.2629 - accuracy: 0.9058 - val_loss: 0.2444 - val_accuracy: 0.9134\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.2286 - accuracy: 0.9164 - val_loss: 0.2326 - val_accuracy: 0.9125\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.1974 - accuracy: 0.9273 - val_loss: 0.2274 - val_accuracy: 0.9160\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1782 - accuracy: 0.9347 - val_loss: 0.2214 - val_accuracy: 0.9224\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.1563 - accuracy: 0.9417 - val_loss: 0.2431 - val_accuracy: 0.9120\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1424 - accuracy: 0.9464 - val_loss: 0.2293 - val_accuracy: 0.9238\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.1254 - accuracy: 0.9539 - val_loss: 0.2306 - val_accuracy: 0.9237\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1118 - accuracy: 0.9579 - val_loss: 0.2568 - val_accuracy: 0.9219\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.5129 - accuracy: 0.8140 - val_loss: 0.3194 - val_accuracy: 0.8831\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.3237 - accuracy: 0.8826 - val_loss: 0.2665 - val_accuracy: 0.8991\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.2717 - accuracy: 0.9018 - val_loss: 0.2502 - val_accuracy: 0.9085\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.2375 - accuracy: 0.9130 - val_loss: 0.2517 - val_accuracy: 0.9081\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.2086 - accuracy: 0.9226 - val_loss: 0.2249 - val_accuracy: 0.9177\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1840 - accuracy: 0.9318 - val_loss: 0.2271 - val_accuracy: 0.9215\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1670 - accuracy: 0.9382 - val_loss: 0.2274 - val_accuracy: 0.9191\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1467 - accuracy: 0.9456 - val_loss: 0.2422 - val_accuracy: 0.9202\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1320 - accuracy: 0.9516 - val_loss: 0.2506 - val_accuracy: 0.9208\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1204 - accuracy: 0.9550 - val_loss: 0.2524 - val_accuracy: 0.9238\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.5086 - accuracy: 0.8130 - val_loss: 0.3036 - val_accuracy: 0.8923\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.3130 - accuracy: 0.8854 - val_loss: 0.2652 - val_accuracy: 0.9044\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.2624 - accuracy: 0.9041 - val_loss: 0.2495 - val_accuracy: 0.9091\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.2301 - accuracy: 0.9151 - val_loss: 0.2337 - val_accuracy: 0.9156\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.2018 - accuracy: 0.9253 - val_loss: 0.2370 - val_accuracy: 0.9160\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1786 - accuracy: 0.9326 - val_loss: 0.2306 - val_accuracy: 0.9127\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1559 - accuracy: 0.9422 - val_loss: 0.2281 - val_accuracy: 0.9218\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1385 - accuracy: 0.9494 - val_loss: 0.2232 - val_accuracy: 0.9244\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1247 - accuracy: 0.9545 - val_loss: 0.2495 - val_accuracy: 0.9224\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1112 - accuracy: 0.9587 - val_loss: 0.2464 - val_accuracy: 0.9250\n",
      "313/313 [==============================] - 3s 8ms/step\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.5049 - accuracy: 0.8157 - val_loss: 0.3186 - val_accuracy: 0.8849\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.3146 - accuracy: 0.8860 - val_loss: 0.2761 - val_accuracy: 0.8997\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.2623 - accuracy: 0.9040 - val_loss: 0.2569 - val_accuracy: 0.9065\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.2290 - accuracy: 0.9144 - val_loss: 0.2310 - val_accuracy: 0.9141\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.2035 - accuracy: 0.9249 - val_loss: 0.2359 - val_accuracy: 0.9146\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1777 - accuracy: 0.9350 - val_loss: 0.2199 - val_accuracy: 0.9220\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.1604 - accuracy: 0.9414 - val_loss: 0.2401 - val_accuracy: 0.9204\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1425 - accuracy: 0.9460 - val_loss: 0.2605 - val_accuracy: 0.9155\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.1292 - accuracy: 0.9512 - val_loss: 0.2661 - val_accuracy: 0.9217\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 43s 35ms/step - loss: 0.1174 - accuracy: 0.9563 - val_loss: 0.2544 - val_accuracy: 0.9241\n",
      "313/313 [==============================] - 3s 9ms/step\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 44s 35ms/step - loss: 0.5026 - accuracy: 0.8175 - val_loss: 0.3271 - val_accuracy: 0.8805\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.3131 - accuracy: 0.8871 - val_loss: 0.2551 - val_accuracy: 0.9076\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.2680 - accuracy: 0.9015 - val_loss: 0.2453 - val_accuracy: 0.9124\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.2317 - accuracy: 0.9159 - val_loss: 0.2321 - val_accuracy: 0.9152\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.2074 - accuracy: 0.9244 - val_loss: 0.2357 - val_accuracy: 0.9149\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.1827 - accuracy: 0.9318 - val_loss: 0.2247 - val_accuracy: 0.9177\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.1626 - accuracy: 0.9413 - val_loss: 0.2449 - val_accuracy: 0.9175\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.1437 - accuracy: 0.9474 - val_loss: 0.2341 - val_accuracy: 0.9189\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.1308 - accuracy: 0.9513 - val_loss: 0.2547 - val_accuracy: 0.9216\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.1175 - accuracy: 0.9572 - val_loss: 0.2520 - val_accuracy: 0.9201\n",
      "313/313 [==============================] - 3s 9ms/step\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 55s 35ms/step - loss: 0.4831 - accuracy: 0.8217 - val_loss: 0.3111 - val_accuracy: 0.8871\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.3104 - accuracy: 0.8857 - val_loss: 0.2722 - val_accuracy: 0.9024\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.2624 - accuracy: 0.9040 - val_loss: 0.2540 - val_accuracy: 0.9067\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.2303 - accuracy: 0.9152 - val_loss: 0.2643 - val_accuracy: 0.9035\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.2063 - accuracy: 0.9232 - val_loss: 0.2204 - val_accuracy: 0.9186\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.1849 - accuracy: 0.9315 - val_loss: 0.2249 - val_accuracy: 0.9168\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.1688 - accuracy: 0.9377 - val_loss: 0.2253 - val_accuracy: 0.9186\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.1502 - accuracy: 0.9433 - val_loss: 0.2372 - val_accuracy: 0.9216\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.1371 - accuracy: 0.9485 - val_loss: 0.2398 - val_accuracy: 0.9222\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.1229 - accuracy: 0.9537 - val_loss: 0.2530 - val_accuracy: 0.9212\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(model=&lt;function build_model at 0x7ff1e252a310&gt;, n_neurons=100, opt_type=&#x27;sgd&#x27;),\n",
       "             param_grid={&#x27;n_neurons&#x27;: [100, 200], &#x27;opt_type&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(model=&lt;function build_model at 0x7ff1e252a310&gt;, n_neurons=100, opt_type=&#x27;sgd&#x27;),\n",
       "             param_grid={&#x27;n_neurons&#x27;: [100, 200], &#x27;opt_type&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=&lt;function build_model at 0x7ff1e252a310&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=None\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=1\n",
       "\tn_neurons=100\n",
       "\topt_type=sgd\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=&lt;function build_model at 0x7ff1e252a310&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=None\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=1\n",
       "\tn_neurons=100\n",
       "\topt_type=sgd\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=KerasClassifier(model=<function build_model at 0x7ff1e252a310>, n_neurons=100, opt_type='sgd'),\n",
       "             param_grid={'n_neurons': [100, 200], 'opt_type': ['sgd', 'adam']})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(keras_classifier, parameters, cv=5)\n",
    "\n",
    "grid_search.fit(X_train, \n",
    "                y_train, \n",
    "                epochs=10, \n",
    "                validation_data = (X_valid, y_valid), \n",
    "                callbacks=EarlyStopping(patience=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffe563",
   "metadata": {},
   "source": [
    "Having finished the hyperparameter fine tuning, the best parameters determined are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22ae5783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neurons': 200, 'opt_type': 'adam'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a2be8c",
   "metadata": {},
   "source": [
    "## Optimial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e395b31",
   "metadata": {},
   "source": [
    "Using this optimal model, we finally assess the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc5eff59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_neurons: 200, opt_type: adam\n"
     ]
    }
   ],
   "source": [
    "optimal_model = build_model(grid_search.best_params_['n_neurons'], grid_search.best_params_['opt_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a51dfad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_44 (Conv2D)          (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_44 (MaxPoolin  (None, 14, 14, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_45 (MaxPoolin  (None, 7, 7, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_22 (Flatten)        (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 200)               627400    \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 688,426\n",
      "Trainable params: 688,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimal_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb2878ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.4751 - accuracy: 0.8283 - val_loss: 0.3074 - val_accuracy: 0.8928\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.3067 - accuracy: 0.8877 - val_loss: 0.2583 - val_accuracy: 0.9031\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.2607 - accuracy: 0.9044 - val_loss: 0.2499 - val_accuracy: 0.9100\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 49s 32ms/step - loss: 0.2293 - accuracy: 0.9171 - val_loss: 0.2312 - val_accuracy: 0.9173\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.2017 - accuracy: 0.9245 - val_loss: 0.2196 - val_accuracy: 0.9226\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.1810 - accuracy: 0.9325 - val_loss: 0.2213 - val_accuracy: 0.9174\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.1621 - accuracy: 0.9386 - val_loss: 0.2342 - val_accuracy: 0.9181\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 51s 32ms/step - loss: 0.1457 - accuracy: 0.9451 - val_loss: 0.2214 - val_accuracy: 0.9218\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 51s 32ms/step - loss: 0.1330 - accuracy: 0.9501 - val_loss: 0.2451 - val_accuracy: 0.9215\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.1208 - accuracy: 0.9547 - val_loss: 0.2488 - val_accuracy: 0.9233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff1b07b42e0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_model.fit(X_train, y_train, epochs=10, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40dba60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 0.2889 - accuracy: 0.9172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28894561529159546, 0.9172000288963318]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38ccee",
   "metadata": {},
   "source": [
    "As we can see, this has achieved a slightly better performance on the test dataset. Overall, we achieved very strong classification performance on the MNIST Fashion Dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
