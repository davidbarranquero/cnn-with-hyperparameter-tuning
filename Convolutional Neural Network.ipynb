{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a6db2cd",
   "metadata": {},
   "source": [
    "## David Barranquero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f30e2fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 15:39:20.751781: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb57c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba00a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ee764",
   "metadata": {},
   "source": [
    "Setting a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ceaab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 54321\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90e096",
   "metadata": {},
   "source": [
    "We're going to build a convolutional neural network for image classification. The dataset we will use is the MNIST Fashion Dataset, which contains 60,000 training images and 10,000 test images, all 28x28 and in greyscale. The dataset contains an equal amount of images of 10 different clothing articles. We begin first by reading in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfba2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e8379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \n",
    "               \"Trouser\", \n",
    "               \"Pullover\", \n",
    "               \"Dress\", \n",
    "               \"Coat\", \n",
    "               \"Sandal\", \n",
    "               \"Shirt\", \n",
    "               \"Sneaker\", \n",
    "               \"Bag\", \n",
    "               \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d163b6e",
   "metadata": {},
   "source": [
    "Next, we visualise our dataset, viewing the first entry in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8493d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ankle boot\n"
     ]
    }
   ],
   "source": [
    "print(class_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba0a0e",
   "metadata": {},
   "source": [
    "As well as visualising the image itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25109b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7eba7083d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4d2b9",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a258c",
   "metadata": {},
   "source": [
    "Firstly, we must perform pre-processing on the dataset. We are going to employ Min-Max normalisation on the dataset to place all values between 0 and 1. By normalising, the network will achieve a faster convergence to the local minimum. Fortunately, because of the 256 pixel values ranging from 0 to 255, the normalisation effect is achieved by simply dividing the dataset by 255. To do so, we first convert the dataset type to float, and then perform the normalisation. Furthermore, as general practice for when we deal with colour images and have three RGB colour channels, we reshape the dataset to include this information. We define a function for this preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d8cb2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_data(dataset):\n",
    "    dataset = dataset.reshape((dataset.shape[0], 28, 28, 1))\n",
    "    dataset = dataset.astype('float32')\n",
    "    dataset /= 255\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b00a8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalise_data(X_train)\n",
    "X_test = normalise_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5b543",
   "metadata": {},
   "source": [
    "Furthermore, we define a validation set using the first 10000 images of the training dataset. Ideally, when creating a validation set, it is wise to perform stratified sampling on your dataset. When there are large class imbalances, stratified sampling lowers the sampling variance of dataset split, which translates to better performance for the image classification model. However, if we look at the unique counts of the first 10000 images, we can see that there is a fairly uniform distribution of roughly a thousand per image, so the advantage of stratified sampling is minimal over simple random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada10d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([ 942, 1027, 1016, 1019,  974,  989, 1021, 1022,  990, 1000]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train[:10000], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0000eb1",
   "metadata": {},
   "source": [
    "For this reason, we will use the first 10000 images for our validation set, and the remaining 50000 for our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bfe256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train[:10000], X_train[10000:]\n",
    "y_valid, y_train = y_train[:10000], y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5bd634",
   "metadata": {},
   "source": [
    "Finally, we confirm the set sizes and shapes of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58fcf2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28, 1)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba49bd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1073c466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8023b0a",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208414d",
   "metadata": {},
   "source": [
    "For image classification tasks, convolutional neural networks consistently provide the best performance.\n",
    "\n",
    "Our model will consist of a convolutional layer with 32 filters and the standard 3x3 kernel size, with a stride of 1. We will use a layer of padding to ensure that the output is of the same shape and information is not lost. Finally, for our activation function we will use ReLU. ReLU is defined as ReLU(a) = max(0,a). It leads to faster convergence by promoting sparsity in the model, as well as avoiding the vanishing/exploding gradients issue of other activation functions. \n",
    "\n",
    "Next, we employ a pooling layer using max pooling, again using the standard 2x2 pool size with a step size of 2. Pooling layers are effective at reducing the dimensionality of the dataset while retaining the most important information.\n",
    "\n",
    "Finally, we flatten the dataset in order to use the dense layers of standard feed-forward neural networks for the classification. We will use a layer with 100 layers, and the ReLU activation. Finally, we add an output layer, with the softmax activation, as this is a multi-class classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e2f6143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 15:40:20.885955: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3,3), strides=1, padding=\"same\", activation=\"relu\", input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bb7505",
   "metadata": {},
   "source": [
    "Our loss function for this model will be sparse categorical crossentropy, as the target variable contains only the sparse label vector position. We use stochastic gradient descent as our optimiser, which aids in speeding up the convergence to the local minima by taking faster, imprecise steps, rather than slow, precise steps of gradient descent. Finally, we use accuracy as our performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "459eea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f843930",
   "metadata": {},
   "source": [
    "We can display a summary of our model as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e4e206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               627300    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 628,630\n",
      "Trainable params: 628,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f36dea",
   "metadata": {},
   "source": [
    "Next, we fit the model and assess it's performance on the validation set. We will perform 10 sweeps over the entire dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42257441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.7293 - accuracy: 0.7483 - val_loss: 0.5478 - val_accuracy: 0.8049\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4893 - accuracy: 0.8256 - val_loss: 0.5032 - val_accuracy: 0.8230\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4380 - accuracy: 0.8448 - val_loss: 0.4208 - val_accuracy: 0.8501\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4032 - accuracy: 0.8571 - val_loss: 0.4050 - val_accuracy: 0.8591\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3775 - accuracy: 0.8656 - val_loss: 0.3752 - val_accuracy: 0.8681\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3580 - accuracy: 0.8719 - val_loss: 0.3746 - val_accuracy: 0.8683\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.3403 - accuracy: 0.8776 - val_loss: 0.3488 - val_accuracy: 0.8779\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3261 - accuracy: 0.8822 - val_loss: 0.3400 - val_accuracy: 0.8801\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.3145 - accuracy: 0.8872 - val_loss: 0.3125 - val_accuracy: 0.8889\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.3030 - accuracy: 0.8914 - val_loss: 0.3221 - val_accuracy: 0.8889\n"
     ]
    }
   ],
   "source": [
    "model_fit = model.fit(X_train, y_train, epochs=10, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6358f",
   "metadata": {},
   "source": [
    "And plot the performance of this model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4cfdd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(model_fit.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d8361",
   "metadata": {},
   "source": [
    "From the plot above, we can see that the training and validation accuracy has consistently increased and the training and validation loss have consistently decreased, which indicates that our model is learning correctly. Furthermore, the closeness of the training and validation curves indicate that our model is not overfitting, but generalising well to unseen data. Finally, we assess the performance of our model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c59797ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.3413 - accuracy: 0.8774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3412894606590271, 0.8773999810218811]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83016f7",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ab159",
   "metadata": {},
   "source": [
    "Having built our base model, we are now going to fine-tune our hyperparameters. For this we will define a function to build the model, and then use a scikit-learn wrapper so that we can use GridSearchCV for our grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81110e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_neurons, opt_type):\n",
    "    print('n_neurons: {0}, opt_type: {1}'.format(n_neurons, opt_type))\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3,3), strides=1, padding=\"same\", activation=\"relu\", input_shape=(28,28,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt_type, metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4aa605c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f_/3vhcqh5j54n5rh_ffnt1jks80000gn/T/ipykernel_14130/1566543695.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_classifier = keras.wrappers.scikit_learn.KerasClassifier(build_model)\n"
     ]
    }
   ],
   "source": [
    "keras_classifier = keras.wrappers.scikit_learn.KerasClassifier(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93925c0a",
   "metadata": {},
   "source": [
    "While there are numerous hyperparameters we could optimise, for the sake of computational time and complexity, we are only going to optimise two, the number of neurons in the dense layer, and the optimiser used for the model. Adam is another popular optimiser that is very fast and efficient by combining the AdaGrad and RMSProp algorithms, which themselves extend on SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83989e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"n_neurons\": [100, 200],\n",
    "    \"opt_type\": [\"sgd\", \"adam\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e600ec66",
   "metadata": {},
   "source": [
    "For 4 potential hyperparameters, using 5-fold cross validaiton, 10 epochs and roughly 20 seconds training per epoch, the grid search should take just over an hour. However, using the more computationally intensive adam optimiser is likely to extend this runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01d487f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_neurons: 100, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.7690 - accuracy: 0.7366 - val_loss: 0.5145 - val_accuracy: 0.8202\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.5013 - accuracy: 0.8225 - val_loss: 0.5056 - val_accuracy: 0.8183\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 19s 16ms/step - loss: 0.4497 - accuracy: 0.8399 - val_loss: 0.4275 - val_accuracy: 0.8508\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4155 - accuracy: 0.8519 - val_loss: 0.3981 - val_accuracy: 0.8612\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.3933 - accuracy: 0.8623 - val_loss: 0.3881 - val_accuracy: 0.8646\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.3728 - accuracy: 0.8676 - val_loss: 0.3632 - val_accuracy: 0.8713\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.3568 - accuracy: 0.8729 - val_loss: 0.3547 - val_accuracy: 0.8748\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 19s 16ms/step - loss: 0.3421 - accuracy: 0.8783 - val_loss: 0.3478 - val_accuracy: 0.8752\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.3299 - accuracy: 0.8832 - val_loss: 0.3448 - val_accuracy: 0.8757\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.3185 - accuracy: 0.8850 - val_loss: 0.3288 - val_accuracy: 0.8839\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3335 - accuracy: 0.8812\n",
      "n_neurons: 100, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 20s 15ms/step - loss: 0.7701 - accuracy: 0.7368 - val_loss: 0.5616 - val_accuracy: 0.7894\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5189 - accuracy: 0.8123 - val_loss: 0.5274 - val_accuracy: 0.8053\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4663 - accuracy: 0.8332 - val_loss: 0.4355 - val_accuracy: 0.8456\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4329 - accuracy: 0.8464 - val_loss: 0.4121 - val_accuracy: 0.8515\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.4094 - accuracy: 0.8544 - val_loss: 0.4031 - val_accuracy: 0.8554\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.3883 - accuracy: 0.8611 - val_loss: 0.3671 - val_accuracy: 0.8712\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.3718 - accuracy: 0.8673 - val_loss: 0.3778 - val_accuracy: 0.8659\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.3570 - accuracy: 0.8731 - val_loss: 0.3468 - val_accuracy: 0.8780\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.3439 - accuracy: 0.8766 - val_loss: 0.3535 - val_accuracy: 0.8764\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.3327 - accuracy: 0.8793 - val_loss: 0.3348 - val_accuracy: 0.8823\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3384 - accuracy: 0.8850\n",
      "n_neurons: 100, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7612 - accuracy: 0.7389 - val_loss: 0.5749 - val_accuracy: 0.7897\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 19s 16ms/step - loss: 0.5116 - accuracy: 0.8175 - val_loss: 0.4622 - val_accuracy: 0.8346\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.4578 - accuracy: 0.8371 - val_loss: 0.4294 - val_accuracy: 0.8466\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.4219 - accuracy: 0.8507 - val_loss: 0.4076 - val_accuracy: 0.8515\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.3967 - accuracy: 0.8592 - val_loss: 0.3839 - val_accuracy: 0.8659\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.3761 - accuracy: 0.8650 - val_loss: 0.3643 - val_accuracy: 0.8720\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.3589 - accuracy: 0.8719 - val_loss: 0.3588 - val_accuracy: 0.8733\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.3442 - accuracy: 0.8766 - val_loss: 0.3478 - val_accuracy: 0.8766\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.3310 - accuracy: 0.8803 - val_loss: 0.3355 - val_accuracy: 0.8820\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.3206 - accuracy: 0.8837 - val_loss: 0.3330 - val_accuracy: 0.8814\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.3316 - accuracy: 0.8822\n",
      "n_neurons: 100, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 20s 15ms/step - loss: 0.7803 - accuracy: 0.7331 - val_loss: 0.5692 - val_accuracy: 0.7908\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 0.5196 - accuracy: 0.8141 - val_loss: 0.4822 - val_accuracy: 0.8253\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 28s 22ms/step - loss: 0.4665 - accuracy: 0.8325 - val_loss: 0.4282 - val_accuracy: 0.8514\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4304 - accuracy: 0.8479 - val_loss: 0.4224 - val_accuracy: 0.8455\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4049 - accuracy: 0.8570 - val_loss: 0.3893 - val_accuracy: 0.8602\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.3853 - accuracy: 0.8623 - val_loss: 0.4021 - val_accuracy: 0.8531\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 0.3666 - accuracy: 0.8680 - val_loss: 0.3723 - val_accuracy: 0.8665\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.3528 - accuracy: 0.8757 - val_loss: 0.3515 - val_accuracy: 0.8743\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.3408 - accuracy: 0.8780 - val_loss: 0.3669 - val_accuracy: 0.8674\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.3294 - accuracy: 0.8818 - val_loss: 0.3412 - val_accuracy: 0.8775\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3398 - accuracy: 0.8792\n",
      "n_neurons: 100, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7647 - accuracy: 0.7418 - val_loss: 0.5408 - val_accuracy: 0.8121\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.5074 - accuracy: 0.8183 - val_loss: 0.4810 - val_accuracy: 0.8251\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.4530 - accuracy: 0.8404 - val_loss: 0.4261 - val_accuracy: 0.8538\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.4175 - accuracy: 0.8535 - val_loss: 0.3982 - val_accuracy: 0.8608\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 19s 16ms/step - loss: 0.3913 - accuracy: 0.8617 - val_loss: 0.3722 - val_accuracy: 0.8710\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.3721 - accuracy: 0.8677 - val_loss: 0.3993 - val_accuracy: 0.8528\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.3535 - accuracy: 0.8758 - val_loss: 0.3657 - val_accuracy: 0.8713\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.3393 - accuracy: 0.8801 - val_loss: 0.3456 - val_accuracy: 0.8792\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.3287 - accuracy: 0.8831 - val_loss: 0.3419 - val_accuracy: 0.8799\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.3170 - accuracy: 0.8867 - val_loss: 0.3309 - val_accuracy: 0.8822\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.3427 - accuracy: 0.8751\n",
      "n_neurons: 100, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 21s 16ms/step - loss: 0.4169 - accuracy: 0.8522 - val_loss: 0.3102 - val_accuracy: 0.8929\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.2792 - accuracy: 0.8989 - val_loss: 0.2786 - val_accuracy: 0.8996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.2355 - accuracy: 0.9143 - val_loss: 0.2630 - val_accuracy: 0.9042\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 21s 16ms/step - loss: 0.2043 - accuracy: 0.9248 - val_loss: 0.2539 - val_accuracy: 0.9058\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.1728 - accuracy: 0.9386 - val_loss: 0.2627 - val_accuracy: 0.9099\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 21s 16ms/step - loss: 0.1502 - accuracy: 0.9449 - val_loss: 0.2453 - val_accuracy: 0.9140\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.1319 - accuracy: 0.9513 - val_loss: 0.2662 - val_accuracy: 0.9122\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.1113 - accuracy: 0.9597 - val_loss: 0.2635 - val_accuracy: 0.9185\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.0942 - accuracy: 0.9653 - val_loss: 0.2762 - val_accuracy: 0.9158\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.0793 - accuracy: 0.9711 - val_loss: 0.2983 - val_accuracy: 0.9186\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3057 - accuracy: 0.9130\n",
      "n_neurons: 100, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.4220 - accuracy: 0.8504 - val_loss: 0.3226 - val_accuracy: 0.8845\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.2853 - accuracy: 0.8951 - val_loss: 0.2763 - val_accuracy: 0.9016\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.2412 - accuracy: 0.9124 - val_loss: 0.2592 - val_accuracy: 0.9033\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 28s 22ms/step - loss: 0.2080 - accuracy: 0.9231 - val_loss: 0.2611 - val_accuracy: 0.9055\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.1788 - accuracy: 0.9348 - val_loss: 0.2554 - val_accuracy: 0.9101\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.1554 - accuracy: 0.9433 - val_loss: 0.2445 - val_accuracy: 0.9148\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 0.1352 - accuracy: 0.9496 - val_loss: 0.2616 - val_accuracy: 0.9102\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.1149 - accuracy: 0.9584 - val_loss: 0.2545 - val_accuracy: 0.9162\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.0982 - accuracy: 0.9643 - val_loss: 0.2837 - val_accuracy: 0.9122\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.0838 - accuracy: 0.9692 - val_loss: 0.2787 - val_accuracy: 0.9160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2889 - accuracy: 0.9134\n",
      "n_neurons: 100, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.4365 - accuracy: 0.8450 - val_loss: 0.3245 - val_accuracy: 0.8905\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.2926 - accuracy: 0.8936 - val_loss: 0.2781 - val_accuracy: 0.9026\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.2466 - accuracy: 0.9100 - val_loss: 0.2624 - val_accuracy: 0.9064\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.2148 - accuracy: 0.9207 - val_loss: 0.2745 - val_accuracy: 0.9006\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.1872 - accuracy: 0.9297 - val_loss: 0.2587 - val_accuracy: 0.9090\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.1615 - accuracy: 0.9402 - val_loss: 0.2429 - val_accuracy: 0.9132\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.1406 - accuracy: 0.9482 - val_loss: 0.2642 - val_accuracy: 0.9131\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.1231 - accuracy: 0.9542 - val_loss: 0.2629 - val_accuracy: 0.9151\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.1040 - accuracy: 0.9614 - val_loss: 0.2780 - val_accuracy: 0.9100\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.0914 - accuracy: 0.9663 - val_loss: 0.2782 - val_accuracy: 0.9169\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2950 - accuracy: 0.9183\n",
      "n_neurons: 100, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.4162 - accuracy: 0.8518 - val_loss: 0.3193 - val_accuracy: 0.8873\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.2803 - accuracy: 0.8979 - val_loss: 0.2726 - val_accuracy: 0.9037\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.2352 - accuracy: 0.9149 - val_loss: 0.2592 - val_accuracy: 0.9090\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.2020 - accuracy: 0.9260 - val_loss: 0.2576 - val_accuracy: 0.9085\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.1758 - accuracy: 0.9352 - val_loss: 0.2423 - val_accuracy: 0.9149\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.1499 - accuracy: 0.9440 - val_loss: 0.2474 - val_accuracy: 0.9148\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.1278 - accuracy: 0.9522 - val_loss: 0.2527 - val_accuracy: 0.9182\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.1114 - accuracy: 0.9594 - val_loss: 0.2673 - val_accuracy: 0.9169\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 34s 28ms/step - loss: 0.0947 - accuracy: 0.9656 - val_loss: 0.3023 - val_accuracy: 0.9086\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 24s 20ms/step - loss: 0.0804 - accuracy: 0.9701 - val_loss: 0.2909 - val_accuracy: 0.9155\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.3018 - accuracy: 0.9151\n",
      "n_neurons: 100, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.4287 - accuracy: 0.8501 - val_loss: 0.3224 - val_accuracy: 0.8886\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.2894 - accuracy: 0.8953 - val_loss: 0.2861 - val_accuracy: 0.8958\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.2414 - accuracy: 0.9129 - val_loss: 0.2632 - val_accuracy: 0.9043\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.2114 - accuracy: 0.9228 - val_loss: 0.2528 - val_accuracy: 0.9097\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.1826 - accuracy: 0.9320 - val_loss: 0.2645 - val_accuracy: 0.9072\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.1589 - accuracy: 0.9418 - val_loss: 0.2451 - val_accuracy: 0.9150\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.1364 - accuracy: 0.9497 - val_loss: 0.2512 - val_accuracy: 0.9172\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.1167 - accuracy: 0.9581 - val_loss: 0.2779 - val_accuracy: 0.9120\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.1011 - accuracy: 0.9639 - val_loss: 0.2822 - val_accuracy: 0.9139\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 0.0882 - accuracy: 0.9681 - val_loss: 0.2762 - val_accuracy: 0.9188\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3074 - accuracy: 0.9104\n",
      "n_neurons: 200, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.7558 - accuracy: 0.7453 - val_loss: 0.4998 - val_accuracy: 0.8266\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.4900 - accuracy: 0.8266 - val_loss: 0.4859 - val_accuracy: 0.8251\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.4366 - accuracy: 0.8440 - val_loss: 0.4154 - val_accuracy: 0.8539\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4014 - accuracy: 0.8575 - val_loss: 0.3868 - val_accuracy: 0.8632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.3788 - accuracy: 0.8662 - val_loss: 0.3768 - val_accuracy: 0.8677\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.3580 - accuracy: 0.8731 - val_loss: 0.3487 - val_accuracy: 0.8767\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.3433 - accuracy: 0.8780 - val_loss: 0.3429 - val_accuracy: 0.8795\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.3288 - accuracy: 0.8832 - val_loss: 0.3359 - val_accuracy: 0.8814\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.3172 - accuracy: 0.8870 - val_loss: 0.3352 - val_accuracy: 0.8805\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.3063 - accuracy: 0.8895 - val_loss: 0.3186 - val_accuracy: 0.8892\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3219 - accuracy: 0.8849\n",
      "n_neurons: 200, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.7512 - accuracy: 0.7454 - val_loss: 0.5511 - val_accuracy: 0.7970\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5058 - accuracy: 0.8191 - val_loss: 0.5120 - val_accuracy: 0.8149\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4493 - accuracy: 0.8398 - val_loss: 0.4195 - val_accuracy: 0.8522\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.4141 - accuracy: 0.8533 - val_loss: 0.3971 - val_accuracy: 0.8560\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.3905 - accuracy: 0.8613 - val_loss: 0.3836 - val_accuracy: 0.8651\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.3691 - accuracy: 0.8683 - val_loss: 0.3519 - val_accuracy: 0.8764\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.3533 - accuracy: 0.8748 - val_loss: 0.3665 - val_accuracy: 0.8704\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.3390 - accuracy: 0.8784 - val_loss: 0.3362 - val_accuracy: 0.8792\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.3258 - accuracy: 0.8828 - val_loss: 0.3420 - val_accuracy: 0.8782\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.3151 - accuracy: 0.8866 - val_loss: 0.3218 - val_accuracy: 0.8861\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3202 - accuracy: 0.8888\n",
      "n_neurons: 200, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.7638 - accuracy: 0.7393 - val_loss: 0.5575 - val_accuracy: 0.7948\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5070 - accuracy: 0.8193 - val_loss: 0.4559 - val_accuracy: 0.8369\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.4546 - accuracy: 0.8379 - val_loss: 0.4280 - val_accuracy: 0.8485\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.4210 - accuracy: 0.8500 - val_loss: 0.4013 - val_accuracy: 0.8527\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.3957 - accuracy: 0.8600 - val_loss: 0.3800 - val_accuracy: 0.8665\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.3758 - accuracy: 0.8665 - val_loss: 0.3625 - val_accuracy: 0.8723\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.3593 - accuracy: 0.8716 - val_loss: 0.3589 - val_accuracy: 0.8740\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.3449 - accuracy: 0.8773 - val_loss: 0.3438 - val_accuracy: 0.8801\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.3311 - accuracy: 0.8810 - val_loss: 0.3303 - val_accuracy: 0.8857\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.3203 - accuracy: 0.8848 - val_loss: 0.3287 - val_accuracy: 0.8831\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3282 - accuracy: 0.8828\n",
      "n_neurons: 200, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 24s 18ms/step - loss: 0.7488 - accuracy: 0.7464 - val_loss: 0.5351 - val_accuracy: 0.8049\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5009 - accuracy: 0.8199 - val_loss: 0.4680 - val_accuracy: 0.8292\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.4516 - accuracy: 0.8383 - val_loss: 0.4126 - val_accuracy: 0.8557\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.4167 - accuracy: 0.8536 - val_loss: 0.4143 - val_accuracy: 0.8466\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.3906 - accuracy: 0.8625 - val_loss: 0.3736 - val_accuracy: 0.8674\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.3715 - accuracy: 0.8669 - val_loss: 0.3794 - val_accuracy: 0.8633\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.3531 - accuracy: 0.8732 - val_loss: 0.3694 - val_accuracy: 0.8662\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.3400 - accuracy: 0.8792 - val_loss: 0.3434 - val_accuracy: 0.8781\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.3277 - accuracy: 0.8820 - val_loss: 0.3503 - val_accuracy: 0.8762\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.3171 - accuracy: 0.8857 - val_loss: 0.3266 - val_accuracy: 0.8822\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3281 - accuracy: 0.8829\n",
      "n_neurons: 200, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7378 - accuracy: 0.7498 - val_loss: 0.5214 - val_accuracy: 0.8182\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4955 - accuracy: 0.8218 - val_loss: 0.4718 - val_accuracy: 0.8282\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4431 - accuracy: 0.8429 - val_loss: 0.4256 - val_accuracy: 0.8513\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4091 - accuracy: 0.8559 - val_loss: 0.3904 - val_accuracy: 0.8614\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.3839 - accuracy: 0.8633 - val_loss: 0.3658 - val_accuracy: 0.8711\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.3651 - accuracy: 0.8692 - val_loss: 0.3927 - val_accuracy: 0.8570\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.3468 - accuracy: 0.8756 - val_loss: 0.3582 - val_accuracy: 0.8733\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.3327 - accuracy: 0.8813 - val_loss: 0.3392 - val_accuracy: 0.8800\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.3219 - accuracy: 0.8839 - val_loss: 0.3412 - val_accuracy: 0.8805\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.3105 - accuracy: 0.8892 - val_loss: 0.3280 - val_accuracy: 0.8824\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3391 - accuracy: 0.8742\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.4091 - accuracy: 0.8558 - val_loss: 0.3022 - val_accuracy: 0.8941\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.2726 - accuracy: 0.8999 - val_loss: 0.2731 - val_accuracy: 0.9034\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.2250 - accuracy: 0.9174 - val_loss: 0.2540 - val_accuracy: 0.9041\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.1896 - accuracy: 0.9311 - val_loss: 0.2609 - val_accuracy: 0.9054\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.1564 - accuracy: 0.9434 - val_loss: 0.2487 - val_accuracy: 0.9103\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.1341 - accuracy: 0.9512 - val_loss: 0.2444 - val_accuracy: 0.9160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.1141 - accuracy: 0.9570 - val_loss: 0.2646 - val_accuracy: 0.9159\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.0933 - accuracy: 0.9665 - val_loss: 0.2612 - val_accuracy: 0.9158\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.0761 - accuracy: 0.9716 - val_loss: 0.2862 - val_accuracy: 0.9163\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.0648 - accuracy: 0.9768 - val_loss: 0.3027 - val_accuracy: 0.9179\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.3294 - accuracy: 0.9136\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4086 - accuracy: 0.8553 - val_loss: 0.3021 - val_accuracy: 0.8928\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.2722 - accuracy: 0.9007 - val_loss: 0.2641 - val_accuracy: 0.9036\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.2249 - accuracy: 0.9164 - val_loss: 0.2494 - val_accuracy: 0.9064\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.1882 - accuracy: 0.9312 - val_loss: 0.2452 - val_accuracy: 0.9104\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.1565 - accuracy: 0.9421 - val_loss: 0.2474 - val_accuracy: 0.9135\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.1299 - accuracy: 0.9536 - val_loss: 0.2403 - val_accuracy: 0.9195\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.1088 - accuracy: 0.9597 - val_loss: 0.2662 - val_accuracy: 0.9164\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.0892 - accuracy: 0.9672 - val_loss: 0.2591 - val_accuracy: 0.9202\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.0699 - accuracy: 0.9746 - val_loss: 0.3058 - val_accuracy: 0.9143\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.0571 - accuracy: 0.9794 - val_loss: 0.3113 - val_accuracy: 0.9170\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3131 - accuracy: 0.9195\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4073 - accuracy: 0.8550 - val_loss: 0.2958 - val_accuracy: 0.8983\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.2723 - accuracy: 0.8995 - val_loss: 0.2640 - val_accuracy: 0.9046\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.2241 - accuracy: 0.9169 - val_loss: 0.2500 - val_accuracy: 0.9093\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.1905 - accuracy: 0.9302 - val_loss: 0.2695 - val_accuracy: 0.9030\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.1594 - accuracy: 0.9412 - val_loss: 0.2711 - val_accuracy: 0.9069\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.1312 - accuracy: 0.9524 - val_loss: 0.2381 - val_accuracy: 0.9181\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.1082 - accuracy: 0.9602 - val_loss: 0.2527 - val_accuracy: 0.9174\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.0897 - accuracy: 0.9672 - val_loss: 0.2781 - val_accuracy: 0.9170\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.0731 - accuracy: 0.9736 - val_loss: 0.3088 - val_accuracy: 0.9118\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.0582 - accuracy: 0.9792 - val_loss: 0.2955 - val_accuracy: 0.9196\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3230 - accuracy: 0.9184\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.4082 - accuracy: 0.8543 - val_loss: 0.3100 - val_accuracy: 0.8891\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.2725 - accuracy: 0.9004 - val_loss: 0.2604 - val_accuracy: 0.9081\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.2243 - accuracy: 0.9185 - val_loss: 0.2427 - val_accuracy: 0.9121\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.1896 - accuracy: 0.9307 - val_loss: 0.2462 - val_accuracy: 0.9111\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.1614 - accuracy: 0.9401 - val_loss: 0.2577 - val_accuracy: 0.9111\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.1327 - accuracy: 0.9512 - val_loss: 0.2470 - val_accuracy: 0.9147\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.1090 - accuracy: 0.9607 - val_loss: 0.2534 - val_accuracy: 0.9181\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.0912 - accuracy: 0.9665 - val_loss: 0.2766 - val_accuracy: 0.9171\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2975 - accuracy: 0.9151\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.4180 - accuracy: 0.8516 - val_loss: 0.3129 - val_accuracy: 0.8885\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.2777 - accuracy: 0.9000 - val_loss: 0.2822 - val_accuracy: 0.8943\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.2264 - accuracy: 0.9173 - val_loss: 0.2547 - val_accuracy: 0.9068\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.1932 - accuracy: 0.9297 - val_loss: 0.2401 - val_accuracy: 0.9132\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.1631 - accuracy: 0.9395 - val_loss: 0.2489 - val_accuracy: 0.9147\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.1350 - accuracy: 0.9508 - val_loss: 0.2493 - val_accuracy: 0.9148\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.1111 - accuracy: 0.9588 - val_loss: 0.2591 - val_accuracy: 0.9188\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.0916 - accuracy: 0.9665 - val_loss: 0.2743 - val_accuracy: 0.9152\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.0768 - accuracy: 0.9725 - val_loss: 0.3086 - val_accuracy: 0.9089\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3323 - accuracy: 0.9058\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.3951 - accuracy: 0.8580 - val_loss: 0.2908 - val_accuracy: 0.9020\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2675 - accuracy: 0.9010 - val_loss: 0.2596 - val_accuracy: 0.9053\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.2206 - accuracy: 0.9178 - val_loss: 0.2465 - val_accuracy: 0.9093\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1852 - accuracy: 0.9327 - val_loss: 0.2406 - val_accuracy: 0.9139\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1591 - accuracy: 0.9398 - val_loss: 0.2327 - val_accuracy: 0.9193\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.1313 - accuracy: 0.9511 - val_loss: 0.2339 - val_accuracy: 0.9165\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.1098 - accuracy: 0.9599 - val_loss: 0.2424 - val_accuracy: 0.9218\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 0.0893 - accuracy: 0.9678 - val_loss: 0.2617 - val_accuracy: 0.9212\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.0748 - accuracy: 0.9729 - val_loss: 0.2872 - val_accuracy: 0.9187\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 0.0603 - accuracy: 0.9782 - val_loss: 0.2936 - val_accuracy: 0.9191\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=&lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x7f7eb53b7e50&gt;,\n",
       "             param_grid={&#x27;n_neurons&#x27;: [100, 200], &#x27;opt_type&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=&lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x7f7eb53b7e50&gt;,\n",
       "             param_grid={&#x27;n_neurons&#x27;: [100, 200], &#x27;opt_type&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x7f7eb53b7e50&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x7f7eb53b7e50&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7f7eb53b7e50>,\n",
       "             param_grid={'n_neurons': [100, 200], 'opt_type': ['sgd', 'adam']})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(keras_classifier, parameters, cv=5)\n",
    "\n",
    "grid_search.fit(X_train, \n",
    "                y_train, \n",
    "                epochs=10, \n",
    "                validation_data = (X_valid, y_valid), \n",
    "                callbacks=EarlyStopping(patience=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffe563",
   "metadata": {},
   "source": [
    "Having finished the hyperparameter fine tuning, the best parameters determined are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22ae5783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neurons': 200, 'opt_type': 'adam'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3996e64",
   "metadata": {},
   "source": [
    "These parameters have achieved the following validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e5bea69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9144799947738648"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a2be8c",
   "metadata": {},
   "source": [
    "## Optimial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e395b31",
   "metadata": {},
   "source": [
    "Using this optimal model, we finally assess the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc5eff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = grid_search.best_estimator_.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a51dfad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_21 (Conv2D)          (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, 14, 14, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_21 (Flatten)        (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 200)               1254600   \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,256,930\n",
      "Trainable params: 1,256,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimal_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40dba60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3319 - accuracy: 0.9132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33188846707344055, 0.9132000207901001]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38ccee",
   "metadata": {},
   "source": [
    "As we can see, this has achieved a slightly better performance on the test dataset. Overall, we achieved very strong classification performance on the MNIST Fashion Dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
