{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a6db2cd",
   "metadata": {},
   "source": [
    "## David Barranquero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f30e2fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 14:14:37.300061: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb57c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba00a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ee764",
   "metadata": {},
   "source": [
    "Setting a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ceaab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 54321\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90e096",
   "metadata": {},
   "source": [
    "We're going to build a convolutional neural network for image classification. The dataset we will use is the MNIST Fashion Dataset, which contains 60,000 training images and 10,000 test images, all 28x28 and in greyscale. The dataset contains an equal amount of images of 10 different clothing articles. We begin first by reading in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfba2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e8379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \n",
    "               \"Trouser\", \n",
    "               \"Pullover\", \n",
    "               \"Dress\", \n",
    "               \"Coat\", \n",
    "               \"Sandal\", \n",
    "               \"Shirt\", \n",
    "               \"Sneaker\", \n",
    "               \"Bag\", \n",
    "               \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d163b6e",
   "metadata": {},
   "source": [
    "Next, we visualise our dataset, viewing the first entry in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8493d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ankle boot\n"
     ]
    }
   ],
   "source": [
    "print(class_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba0a0e",
   "metadata": {},
   "source": [
    "As well as visualising the image itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25109b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcf9f7084c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAASdklEQVR4nO3da4xVZZYG4HcBhchNQbC4FPerlwiNRzIKUSbtEPGH0DGaJqZDJ0T6h8bu2D9GnRhMDAmZTNPpxEkbesSmJyhp0y0SNTM4SEKI0HJUWu6iWFyKgqqigAKU+5ofte2UWHut8uxzk/U+Camqs853zlenfN1VZ+1vf6KqIKJrX7dKT4CIyoNhJwqCYScKgmEnCoJhJwqiRzmfbNCgQTp69OhyPiVRKPX19WhpaZHOapnCLiIPAPgdgO4A/ktVl1r3Hz16NPL5fJanJCJDLpdLrRX8a7yIdAfwnwDmALgVwHwRubXQxyOi0sryN/t0AJ+r6n5VvQBgNYC5xZkWERVblrAPB3Cow9eHk9u+RUQWiUheRPLNzc0Zno6Isij5u/GqulxVc6qaGzx4cKmfjohSZAl7A4ARHb6uS24joiqUJexbAUwQkTEi0hPATwGsLc60iKjYCm69qeolEXkSwP+ivfW2QlV3Fm1mRFRUmfrsqvougHeLNBciKiGeLksUBMNOFATDThQEw04UBMNOFATDThQEw04UBMNOFATDThQEw04UBMNOFATDThQEw04URFkvJU3l523cKdLpVYe77Pz582Z9z549qbUpU6Zkem7ve7Pq3bpV9jiXZUPVQn9mPLITBcGwEwXBsBMFwbATBcGwEwXBsBMFwbATBcE++zUua5+9tbXVrL/66qtmvXfv3gXVAKBnz55mfdSoUWY9yzkEWXr4XZGlz3/lypXCnrPgZySiHxSGnSgIhp0oCIadKAiGnSgIhp0oCIadKAj22a9xWfvBW7ZsMetvv/22WR8zZkxq7dy5c+bYs2fPmvUhQ4aY9fnz56fW+vTpY471evRZrwNw4cKFgh+7pqamoOfMFHYRqQdwGsBlAJdUNZfl8YiodIpxZP9nVW0pwuMQUQnxb3aiILKGXQGsE5GPRGRRZ3cQkUUikheRfHNzc8anI6JCZQ37TFWdBmAOgCdE5N6r76Cqy1U1p6q5wYMHZ3w6IipUprCrakPysQnAmwCmF2NSRFR8BYddRPqISL9vPgcwG8COYk2MiIory7vxtQDeTHqCPQC8pqr/U5RZUdF079490/iNGzea9V27dpn1ixcvpta8ddnz5s0z65s3bzbrzz//fGptxowZ5tjbb7/drNfV1Zn1vXv3mvUPPvggtXbvvd/5a/hbJk6cmFqzzqsoOOyquh9Atqv8E1HZsPVGFATDThQEw04UBMNOFATDThQEl7heA6x2i7dccufOnWZ906ZNZv2GG24w66dOnUqtbdu2zRzr1WfNmmXWJ02alFqz5gX433dDQ4NZ9y6DPXPmzNTaSy+9ZI59+umnU2vWFto8shMFwbATBcGwEwXBsBMFwbATBcGwEwXBsBMFIVkvNfx95HI5zefzZXu+H4pS/gy8Pvvs2bPNuteH91jfm3dJ5Ouuuy7Tc1uXi/aW/npLYCdPnmzWve9tzZo1qbXt27ebYw8cOJBay+VyyOfznf7QeWQnCoJhJwqCYScKgmEnCoJhJwqCYScKgmEnCoLr2atA1u1/s/B26enVq5dZ79evn1n/6quvUmvWtsUA0NbWZtavv/56s3769OnUmtdnf+edd8z6unXrzPrly5fN+pEjR1Jr1lbTWfDIThQEw04UBMNOFATDThQEw04UBMNOFATDThQE++zBnT171qx7/WKv3r9//9Sa1+P36rt37zbrVi/du4aA93155wD06GFHq1u39OPs/v37zbGFco/sIrJCRJpEZEeH2waKyHsisi/5OKAksyOiounKr/F/BPDAVbc9A2C9qk4AsD75moiqmBt2Vd0IoPWqm+cCWJl8vhLAvOJOi4iKrdA36GpVtTH5/CiA2rQ7isgiEcmLSL65ubnApyOirDK/G6/t73SkvtuhqstVNaeqOe8NFyIqnULDfkxEhgJA8rGpeFMiolIoNOxrASxIPl8A4K3iTIeISsXts4vI6wBmARgkIocBLAawFMCfRWQhgAMAHi3lJK91Xs/Xq1s9W2/N+L59+8x67969zbq33v3cuXMFj+3bt69Zb2lpMevDhg1LrXl98q+//tqsDxhgd5uPHz9u1q392U+cOGGOPXjwYGrN+nm7YVfVtJX0P/bGElH14OmyREEw7ERBMOxEQTDsREEw7ERBcIlrFfAuJX3lypWCH3vDhg1m3WrjAHb7CvCXyFrLTE+dOmWOtdp2gN+6sy5j7W0H7bUsve+7qck+z2zx4sWpta1bt5pjreW3VpuWR3aiIBh2oiAYdqIgGHaiIBh2oiAYdqIgGHaiINhnrwJeH93bXtgyadIks+4tYT1//rxZ9+ZuLb9taGgwx3pbMg8dOtSsW3P3+uTWds+Af5nrsWPHmvWXX345tbZ06VJz7JgxY1Jr1vkDPLITBcGwEwXBsBMFwbATBcGwEwXBsBMFwbATBfGD6rNba3WzXo7Zq1u9bm89usfqRWd11113mfV+/fqZde9yzt6ac+u18frkly5dMuter9xbs27p2bOnWffOffDmvmXLltSa9zMpFI/sREEw7ERBMOxEQTDsREEw7ERBMOxEQTDsREFUVZ89y9rorL3uSvK2TV69erVZf//991Nrffr0Mcd614X3+ugXL1406z16pP8n1r9/f3Os16u2rgsPAGfOnEmteec2eOcXeLwtn63Hf+2118yx06ZNK2hO7pFdRFaISJOI7Ohw2wsi0iAi25J/Dxb07ERUNl35Nf6PAB7o5PbfqurU5N+7xZ0WERWbG3ZV3QigtQxzIaISyvIG3ZMi8mnya/6AtDuJyCIRyYtIvrm5OcPTEVEWhYb99wDGAZgKoBHAb9LuqKrLVTWnqjnvIn1EVDoFhV1Vj6nqZVW9AuAPAKYXd1pEVGwFhV1EOq5N/AmAHWn3JaLq4PbZReR1ALMADBKRwwAWA5glIlMBKIB6AL8oxmRKua7b63t6e4UfOHAgtdbY2GiOXbVqlVn39uP2ru1u7dft9bKPHDli1sePH2/WvT6+1ac/dOiQOdZbU+6tZ58zZ05qzerBA8CaNWvMureefcCA1LexANhr7devX2+OLZQbdlWd38nNr5RgLkRUQjxdligIhp0oCIadKAiGnSgIhp0oiKpa4rp//36z/uyzz6bWDh8+bI49duyYWa+pqTHr1lLO2tpac6zXQho4cKBZ97YutpYGe5clvuOOO8y6tbUwANx///1mvbU1fVlFr169zLHe0l/P5s2bU2snT540x44bN86sey1Nb8tnq9X72WefmWMLxSM7URAMO1EQDDtREAw7URAMO1EQDDtREAw7URBl77NbPeHHH3/cHPvFF1+k1qxLFgN+H93rm1q85bPe3LJu0Wtd7mvv3r3m2CVLlph1b3ntiy++aNZHjhxZ8GM/8sgjZt3rhVv96oaGBnOsd26Dd4lta9kxYP/3OGTIEHNsoXhkJwqCYScKgmEnCoJhJwqCYScKgmEnCoJhJwqirH32trY28zK5u3fvNsdPmTIltXbixAlzrFc/evSoWbdcuHDBrO/cudOse/3iCRMmmPW2trbUWl1dnTl29uzZZt1aEw4ADz/8sFmvr69PrVnzBoAtW7aY9bVr15p165wOby29tx2012f3WOdeeNtgW6+b1d/nkZ0oCIadKAiGnSgIhp0oCIadKAiGnSgIhp0oiLL22Xv06IHBgwen1idNmmSOb2lpSa317dvXHOutEfb68FZf1ZoX4F9X/pZbbjHr3nbS1np4b0tl75r299xzj1mfMWOGWd+xY0dqzVqHD9jbGgPATTfdVPB47xoDXh/+/PnzZt3b0llVU2veeRvWWnyrR+8e2UVkhIhsEJFdIrJTRH6Z3D5QRN4TkX3JR3tDaiKqqK78Gn8JwK9V9VYA/wTgCRG5FcAzANar6gQA65OviahKuWFX1UZV/Tj5/DSA3QCGA5gLYGVyt5UA5pVojkRUBN/rDToRGQ3gRwD+BqBWVRuT0lEAnf5hKiKLRCQvInlvfy0iKp0uh11E+gL4C4Bfqeq3zsTX9ncbOn3HQVWXq2pOVXM33nhjlrkSUQZdCruI1KA96KtU9a/JzcdEZGhSHwqgqTRTJKJicFtvIiIAXgGwW1WXdSitBbAAwNLk41veY9XU1Jitt/anSjdx4sTU2pkzZ8yx3pbON998s1kfNmxYam3EiBHmWG/Jordc0mvzWN/78ePHzbHWMlDAb1l++OGHZt1qiY4fPz7Tc3vLUK2fmXdp8ayXJvcuL37w4MHUmtWWA4BPPvkktWa9Jl3ps88A8DMA20VkW3Lbc2gP+Z9FZCGAAwAe7cJjEVGFuGFX1U0A0g65Py7udIioVHi6LFEQDDtREAw7URAMO1EQDDtREGVd4lpTU4Phw4en1h977DFz/LJly1Jr3uWWb7vtNrPuLWm0etlen/zs2bNm3evJXrp0yaxbWx97/WDv3AZvK+uxY8eadWupp9fL9pZ6WudsAPbSYO/nPWCAvYjTq3tLh63XzbukupUh6+fNIztREAw7URAMO1EQDDtREAw7URAMO1EQDDtREGXts3sWLlxo1u+8887U2pIlS8yxu3btMusjR44069ZVdrzLNVvb6AJ+P9nrs1uP762N9vrs3ty8tfbWOQbe+Qne3D3W+FGjRpljvesjeNcJ6NbNPo5++eWXqbW7777bHHvfffel1qzLivPIThQEw04UBMNOFATDThQEw04UBMNOFATDThRE2fvsVu/T6/lOnTo1tfbGG2+YY/fs2WPWn3rqKbNubT3c2tpqjvWuze714b3rzltrxr1edV1dnVnPci1/wF5r722z7b0uHmvu3jp/79wJ72f60EMPmXXr+gveNQIKxSM7URAMO1EQDDtREAw7URAMO1EQDDtREAw7URBd2Z99BIA/AagFoACWq+rvROQFAI8DaE7u+pyqvtuFxyt8thlMnjzZrK9bt67gx25ubjbrJ0+eNOvWGmQAaGpqMuvWPubetdkHDhxo1una0ZWTai4B+LWqfiwi/QB8JCLvJbXfqup/lG56RFQsXdmfvRFAY/L5aRHZDSB9Swoiqkrf6292ERkN4EcA/pbc9KSIfCoiK0Sk0/1wRGSRiORFJO/9uktEpdPlsItIXwB/AfArVW0D8HsA4wBMRfuR/zedjVPV5aqaU9WctzcXEZVOl8IuIjVoD/oqVf0rAKjqMVW9rKpXAPwBwPTSTZOIsnLDLu1vn78CYLeqLutw+9AOd/sJgPRlYURUcV15N34GgJ8B2C4i25LbngMwX0Smor0dVw/gFyWY3w+C9+dJ1j9frNYaUVd15d34TQA6a467PXUiqh48g44oCIadKAiGnSgIhp0oCIadKAiGnSgIhp0oCIadKAiGnSgIhp0oCIadKAiGnSgIhp0oCIadKAjxtvQt6pOJNAM40OGmQQBayjaB76da51at8wI4t0IVc26jVLXTCyiUNezfeXKRvKrmKjYBQ7XOrVrnBXBuhSrX3PhrPFEQDDtREJUO+/IKP7+lWudWrfMCOLdClWVuFf2bnYjKp9JHdiIqE4adKIiKhF1EHhCRvSLyuYg8U4k5pBGRehHZLiLbRCRf4bmsEJEmEdnR4baBIvKeiOxLPna6x16F5vaCiDQkr902EXmwQnMbISIbRGSXiOwUkV8mt1f0tTPmVZbXrex/s4tIdwCfAfgXAIcBbAUwX1V3lXUiKUSkHkBOVSt+AoaI3AvgDIA/qertyW3/DqBVVZcm/6McoKr/WiVzewHAmUpv453sVjS04zbjAOYB+Dkq+NoZ83oUZXjdKnFknw7gc1Xdr6oXAKwGMLcC86h6qroRQOtVN88FsDL5fCXa/2Mpu5S5VQVVbVTVj5PPTwP4Zpvxir52xrzKohJhHw7gUIevD6O69ntXAOtE5CMRWVTpyXSiVlUbk8+PAqit5GQ64W7jXU5XbTNeNa9dIdufZ8U36L5rpqpOAzAHwBPJr6tVSdv/Bqum3mmXtvEul062Gf+HSr52hW5/nlUlwt4AYESHr+uS26qCqjYkH5sAvInq24r62Dc76CYfmyo8n3+opm28O9tmHFXw2lVy+/NKhH0rgAkiMkZEegL4KYC1FZjHd4hIn+SNE4hIHwCzUX1bUa8FsCD5fAGAtyo4l2+plm2807YZR4Vfu4pvf66qZf8H4EG0vyP/BYB/q8QcUuY1FsDfk387Kz03AK+j/de6i2h/b2MhgJsArAewD8D/ARhYRXP7bwDbAXyK9mANrdDcZqL9V/RPAWxL/j1Y6dfOmFdZXjeeLksUBN+gIwqCYScKgmEnCoJhJwqCYScKgmEnCoJhJwri/wEAWB+BNM85DgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4d2b9",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a258c",
   "metadata": {},
   "source": [
    "Firstly, we must perform pre-processing on the dataset. We are going to employ Min-Max normalisation on the dataset to place all values between 0 and 1. By normalising, the network will achieve a faster convergence to the local minimum. Fortunately, because of the 256 pixel values ranging from 0 to 255, the normalisation effect is achieved by simply dividing the dataset by 255. To do so, we first convert the dataset type to float, and then perform the normalisation. Furthermore, as general practice for when we deal with colour images and have three RGB colour channels, we reshape the dataset to include this information. We define a function for this preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d8cb2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_data(dataset):\n",
    "    dataset = dataset.reshape((dataset.shape[0], 28, 28, 1))\n",
    "    dataset = dataset.astype('float32')\n",
    "    dataset /= 255\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b00a8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalise_data(X_train)\n",
    "X_test = normalise_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5b543",
   "metadata": {},
   "source": [
    "Furthermore, we define a validation set using the first 10000 images of the training dataset. Ideally, when creating a validation set, it is wise to perform stratified sampling on your dataset. When there are large class imbalances, stratified sampling lowers the sampling variance of dataset split, which translates to better performance for the image classification model. However, if we look at the unique counts of the first 10000 images, we can see that there is a fairly uniform distribution of roughly a thousand per image, so the advantage of stratified sampling is minimal over simple random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada10d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([ 942, 1027, 1016, 1019,  974,  989, 1021, 1022,  990, 1000]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train[:10000], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0000eb1",
   "metadata": {},
   "source": [
    "For this reason, we will use the first 10000 images for our validation set, and the remaining 50000 for our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bfe256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train[:10000], X_train[10000:]\n",
    "y_valid, y_train = y_train[:10000], y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5bd634",
   "metadata": {},
   "source": [
    "Finally, we confirm the set sizes and shapes of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58fcf2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28, 1)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba49bd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1073c466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8023b0a",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208414d",
   "metadata": {},
   "source": [
    "For image classification tasks, convolutional neural networks consistently provide the best performance.\n",
    "\n",
    "Our model will consist of a convolutional layer with 32 filters and the standard 3x3 kernel size, with a stride of 1. We will use a layer of padding to ensure that the output is of the same shape and information is not lost. Finally, for our activation function we will use ReLU. ReLU is defined as ReLU(a) = max(0,a). It leads to faster convergence by promoting sparsity in the model, as well as avoiding the vanishing/exploding gradients issue of other activation functions. \n",
    "\n",
    "Next, we employ a pooling layer using max pooling, again using the standard 2x2 pool size with a step size of 2. Pooling layers are effective at reducing the dimensionality of the dataset while retaining the most important information.\n",
    "\n",
    "This convolution and pooling is then repeated again, with 64 kernels, to capture more complex shapes and images.\n",
    "\n",
    "Finally, we flatten the dataset in order to use the dense layers of standard feed-forward neural networks for the classification. We will use two fully connected layers with 100 neurons each, and the ReLU activation. Each will have a dropout layer with a rate of 25% to create a more robust classifier. Finally, we add an output layer, with the softmax activation, as this is a multi-class classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e2f6143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 14:17:10.243049: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3,3), strides=1, padding=\"same\", activation=\"relu\", input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, kernel_size=(3,3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bb7505",
   "metadata": {},
   "source": [
    "Our loss function for this model will be sparse categorical crossentropy, as the target variable contains only the sparse label vector position. We use stochastic gradient descent as our optimiser, which aids in speeding up the convergence to the local minima by taking faster, imprecise steps, rather than slow, precise steps of gradient descent. Finally, we use accuracy as our performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "459eea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f843930",
   "metadata": {},
   "source": [
    "We can display a summary of our model as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e4e206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               313700    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 343,626\n",
      "Trainable params: 343,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f36dea",
   "metadata": {},
   "source": [
    "Next, we fit the model and assess it's performance on the validation set. We will perform 10 sweeps over the entire dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42257441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 1.0849 - accuracy: 0.5939 - val_loss: 0.6462 - val_accuracy: 0.7595\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 0.6622 - accuracy: 0.7531 - val_loss: 0.5186 - val_accuracy: 0.8061\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.5641 - accuracy: 0.7930 - val_loss: 0.4506 - val_accuracy: 0.8353\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.5027 - accuracy: 0.8164 - val_loss: 0.4261 - val_accuracy: 0.8420\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.4644 - accuracy: 0.8325 - val_loss: 0.3799 - val_accuracy: 0.8625\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4359 - accuracy: 0.8434 - val_loss: 0.4020 - val_accuracy: 0.8483\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4117 - accuracy: 0.8506 - val_loss: 0.3511 - val_accuracy: 0.8737\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.3952 - accuracy: 0.8561 - val_loss: 0.3363 - val_accuracy: 0.8769\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.3765 - accuracy: 0.8652 - val_loss: 0.3173 - val_accuracy: 0.8831\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.3640 - accuracy: 0.8684 - val_loss: 0.3131 - val_accuracy: 0.8891\n"
     ]
    }
   ],
   "source": [
    "model_fit = model.fit(X_train, y_train, epochs=10, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6358f",
   "metadata": {},
   "source": [
    "And plot the performance of this model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4cfdd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(model_fit.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d8361",
   "metadata": {},
   "source": [
    "From the plot above, we can see that the training and validation accuracy has consistently increased and the training and validation loss have consistently decreased, which indicates that our model is learning correctly. Furthermore, the closeness of the training and validation curves indicate that our model is not overfitting, but generalising well to unseen data. Finally, we assess the performance of our model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c59797ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 0.3326 - accuracy: 0.8782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3326474130153656, 0.8781999945640564]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83016f7",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ab159",
   "metadata": {},
   "source": [
    "Having built our base model, we are now going to fine-tune our hyperparameters. For this we will define a function to build the model, and then use a scikit-learn wrapper so that we can use GridSearchCV for our grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81110e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_neurons, opt_type):\n",
    "    \n",
    "    print('n_neurons: {0}, opt_type: {1}'.format(n_neurons, opt_type))\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3,3), strides=1, padding=\"same\", activation=\"relu\", input_shape=(28,28,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt_type, metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4aa605c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f_/3vhcqh5j54n5rh_ffnt1jks80000gn/T/ipykernel_16710/1566543695.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_classifier = keras.wrappers.scikit_learn.KerasClassifier(build_model)\n"
     ]
    }
   ],
   "source": [
    "keras_classifier = keras.wrappers.scikit_learn.KerasClassifier(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93925c0a",
   "metadata": {},
   "source": [
    "While there are numerous hyperparameters we could optimise, for the sake of computational time and complexity, we are only going to optimise two, the number of neurons in the dense layer, and the optimiser used for the model. Adaptive Momentum Estimation (Adam) is another popular optimiser and is generally considered one of the best performing optimisers for deep learning. We will compare it's performance to SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83989e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"n_neurons\": [100, 200],\n",
    "    \"opt_type\": [\"sgd\", \"adam\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e600ec66",
   "metadata": {},
   "source": [
    "For 4 potential hyperparameters, using 5-fold cross validaiton, 10 epochs and roughly 45 seconds training per epoch, the grid search should take about 2.5 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01d487f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_neurons: 100, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0956 - accuracy: 0.5967 - val_loss: 0.6356 - val_accuracy: 0.7606\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.6762 - accuracy: 0.7455 - val_loss: 0.5606 - val_accuracy: 0.7760\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 0.5842 - accuracy: 0.7828 - val_loss: 0.4687 - val_accuracy: 0.8243\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.5297 - accuracy: 0.8051 - val_loss: 0.4337 - val_accuracy: 0.8418\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.4910 - accuracy: 0.8228 - val_loss: 0.4181 - val_accuracy: 0.8420\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.4603 - accuracy: 0.8330 - val_loss: 0.3856 - val_accuracy: 0.8562\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.4370 - accuracy: 0.8422 - val_loss: 0.3679 - val_accuracy: 0.8661\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.4171 - accuracy: 0.8508 - val_loss: 0.3545 - val_accuracy: 0.8719\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.4010 - accuracy: 0.8550 - val_loss: 0.3387 - val_accuracy: 0.8759\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.3871 - accuracy: 0.8601 - val_loss: 0.3302 - val_accuracy: 0.8827\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3379 - accuracy: 0.8763\n",
      "n_neurons: 100, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 1.1076 - accuracy: 0.5829 - val_loss: 0.6746 - val_accuracy: 0.7434\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6969 - accuracy: 0.7378 - val_loss: 0.5807 - val_accuracy: 0.7726\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6034 - accuracy: 0.7768 - val_loss: 0.4893 - val_accuracy: 0.8157\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.5439 - accuracy: 0.8000 - val_loss: 0.4475 - val_accuracy: 0.8342\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.5068 - accuracy: 0.8139 - val_loss: 0.4272 - val_accuracy: 0.8393\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.4767 - accuracy: 0.8272 - val_loss: 0.3959 - val_accuracy: 0.8582\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.4563 - accuracy: 0.8348 - val_loss: 0.3973 - val_accuracy: 0.8514\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 0.4349 - accuracy: 0.8432 - val_loss: 0.3726 - val_accuracy: 0.8630\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.4166 - accuracy: 0.8474 - val_loss: 0.3689 - val_accuracy: 0.8656\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.4008 - accuracy: 0.8547 - val_loss: 0.3463 - val_accuracy: 0.8768\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.3504 - accuracy: 0.8743\n",
      "n_neurons: 100, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 1.1470 - accuracy: 0.5831 - val_loss: 0.6390 - val_accuracy: 0.7579\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6828 - accuracy: 0.7447 - val_loss: 0.5443 - val_accuracy: 0.7950\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.5853 - accuracy: 0.7819 - val_loss: 0.4821 - val_accuracy: 0.8232\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.5304 - accuracy: 0.8044 - val_loss: 0.4378 - val_accuracy: 0.8387\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.4899 - accuracy: 0.8211 - val_loss: 0.4276 - val_accuracy: 0.8398\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4632 - accuracy: 0.8324 - val_loss: 0.3958 - val_accuracy: 0.8532\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4380 - accuracy: 0.8405 - val_loss: 0.3771 - val_accuracy: 0.8599\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4166 - accuracy: 0.8488 - val_loss: 0.3593 - val_accuracy: 0.8680\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4014 - accuracy: 0.8543 - val_loss: 0.3486 - val_accuracy: 0.8746\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.3896 - accuracy: 0.8583 - val_loss: 0.3360 - val_accuracy: 0.8781\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3373 - accuracy: 0.8798\n",
      "n_neurons: 100, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 41s 32ms/step - loss: 1.1531 - accuracy: 0.5739 - val_loss: 0.6569 - val_accuracy: 0.7476\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6933 - accuracy: 0.7399 - val_loss: 0.5397 - val_accuracy: 0.7959\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.5956 - accuracy: 0.7801 - val_loss: 0.4940 - val_accuracy: 0.8197\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.5338 - accuracy: 0.8036 - val_loss: 0.4742 - val_accuracy: 0.8166\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4934 - accuracy: 0.8216 - val_loss: 0.4024 - val_accuracy: 0.8521\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.4644 - accuracy: 0.8327 - val_loss: 0.3924 - val_accuracy: 0.8615\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4415 - accuracy: 0.8418 - val_loss: 0.3812 - val_accuracy: 0.8621\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4211 - accuracy: 0.8475 - val_loss: 0.3607 - val_accuracy: 0.8687\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4063 - accuracy: 0.8541 - val_loss: 0.3724 - val_accuracy: 0.8622\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.3926 - accuracy: 0.8584 - val_loss: 0.3402 - val_accuracy: 0.8743\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3427 - accuracy: 0.8723\n",
      "n_neurons: 100, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 1.1666 - accuracy: 0.5648 - val_loss: 0.6736 - val_accuracy: 0.7452\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7044 - accuracy: 0.7363 - val_loss: 0.5524 - val_accuracy: 0.7960\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6021 - accuracy: 0.7764 - val_loss: 0.4930 - val_accuracy: 0.8237\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.5454 - accuracy: 0.8006 - val_loss: 0.4470 - val_accuracy: 0.8395\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.5031 - accuracy: 0.8180 - val_loss: 0.4421 - val_accuracy: 0.8364\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.4737 - accuracy: 0.8291 - val_loss: 0.3974 - val_accuracy: 0.8606\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.4437 - accuracy: 0.8396 - val_loss: 0.4067 - val_accuracy: 0.8482\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.4266 - accuracy: 0.8469 - val_loss: 0.3690 - val_accuracy: 0.8666\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.4076 - accuracy: 0.8520 - val_loss: 0.3539 - val_accuracy: 0.8726\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.3971 - accuracy: 0.8575 - val_loss: 0.3464 - val_accuracy: 0.8771\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.3635 - accuracy: 0.8677\n",
      "n_neurons: 100, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 40s 31ms/step - loss: 0.5837 - accuracy: 0.7873 - val_loss: 0.3446 - val_accuracy: 0.8751\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.3596 - accuracy: 0.8710 - val_loss: 0.2814 - val_accuracy: 0.8966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.3045 - accuracy: 0.8906 - val_loss: 0.2657 - val_accuracy: 0.8990\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.2686 - accuracy: 0.9032 - val_loss: 0.2444 - val_accuracy: 0.9087\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2410 - accuracy: 0.9109 - val_loss: 0.2465 - val_accuracy: 0.9130\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 0.2193 - accuracy: 0.9209 - val_loss: 0.2359 - val_accuracy: 0.9146\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 0.2010 - accuracy: 0.9267 - val_loss: 0.2574 - val_accuracy: 0.9035\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 0.1854 - accuracy: 0.9320 - val_loss: 0.2375 - val_accuracy: 0.9173\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 0.1683 - accuracy: 0.9392 - val_loss: 0.2363 - val_accuracy: 0.9181\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.1592 - accuracy: 0.9408 - val_loss: 0.2426 - val_accuracy: 0.9159\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.2514 - accuracy: 0.9143\n",
      "n_neurons: 100, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.5798 - accuracy: 0.7861 - val_loss: 0.3373 - val_accuracy: 0.8746\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.3591 - accuracy: 0.8716 - val_loss: 0.2857 - val_accuracy: 0.8924\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.3011 - accuracy: 0.8912 - val_loss: 0.2790 - val_accuracy: 0.8962\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.2643 - accuracy: 0.9053 - val_loss: 0.2460 - val_accuracy: 0.9093\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.2367 - accuracy: 0.9122 - val_loss: 0.2424 - val_accuracy: 0.9104\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.2103 - accuracy: 0.9231 - val_loss: 0.2362 - val_accuracy: 0.9135\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.1951 - accuracy: 0.9285 - val_loss: 0.2293 - val_accuracy: 0.9159\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.1758 - accuracy: 0.9345 - val_loss: 0.2372 - val_accuracy: 0.9186\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.1616 - accuracy: 0.9402 - val_loss: 0.2641 - val_accuracy: 0.9136\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.1450 - accuracy: 0.9455 - val_loss: 0.2559 - val_accuracy: 0.9165\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.2514 - accuracy: 0.9171\n",
      "n_neurons: 100, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6034 - accuracy: 0.7760 - val_loss: 0.3510 - val_accuracy: 0.8744\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.3767 - accuracy: 0.8648 - val_loss: 0.2930 - val_accuracy: 0.8904\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.3204 - accuracy: 0.8838 - val_loss: 0.2771 - val_accuracy: 0.9007\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.2825 - accuracy: 0.8977 - val_loss: 0.2680 - val_accuracy: 0.9028\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.2587 - accuracy: 0.9059 - val_loss: 0.2433 - val_accuracy: 0.9129\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2336 - accuracy: 0.9150 - val_loss: 0.2385 - val_accuracy: 0.9146\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2168 - accuracy: 0.9207 - val_loss: 0.2431 - val_accuracy: 0.9149\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2017 - accuracy: 0.9246 - val_loss: 0.2328 - val_accuracy: 0.9200\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.1853 - accuracy: 0.9319 - val_loss: 0.2461 - val_accuracy: 0.9140\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.1737 - accuracy: 0.9360 - val_loss: 0.2260 - val_accuracy: 0.9218\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.2324 - accuracy: 0.9188\n",
      "n_neurons: 100, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.5794 - accuracy: 0.7894 - val_loss: 0.3320 - val_accuracy: 0.8813\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.3554 - accuracy: 0.8726 - val_loss: 0.2868 - val_accuracy: 0.8947\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2984 - accuracy: 0.8929 - val_loss: 0.2572 - val_accuracy: 0.9062\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2626 - accuracy: 0.9039 - val_loss: 0.2582 - val_accuracy: 0.9077\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2394 - accuracy: 0.9126 - val_loss: 0.2356 - val_accuracy: 0.9145\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.2166 - accuracy: 0.9201 - val_loss: 0.2384 - val_accuracy: 0.9130\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.2005 - accuracy: 0.9248 - val_loss: 0.2351 - val_accuracy: 0.9170\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.1853 - accuracy: 0.9317 - val_loss: 0.2523 - val_accuracy: 0.9162\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.1712 - accuracy: 0.9370 - val_loss: 0.2501 - val_accuracy: 0.9159\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.1573 - accuracy: 0.9405 - val_loss: 0.2492 - val_accuracy: 0.9183\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.2654 - accuracy: 0.9148\n",
      "n_neurons: 100, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.5780 - accuracy: 0.7922 - val_loss: 0.3424 - val_accuracy: 0.8754\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 0.3581 - accuracy: 0.8718 - val_loss: 0.2890 - val_accuracy: 0.8933\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.3003 - accuracy: 0.8915 - val_loss: 0.2637 - val_accuracy: 0.9050\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2719 - accuracy: 0.9018 - val_loss: 0.2428 - val_accuracy: 0.9088\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.2486 - accuracy: 0.9090 - val_loss: 0.2302 - val_accuracy: 0.9155\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.2236 - accuracy: 0.9189 - val_loss: 0.2251 - val_accuracy: 0.9173\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.2038 - accuracy: 0.9254 - val_loss: 0.2312 - val_accuracy: 0.9182\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.1901 - accuracy: 0.9288 - val_loss: 0.2274 - val_accuracy: 0.9179\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.1738 - accuracy: 0.9352 - val_loss: 0.2386 - val_accuracy: 0.9180\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.1610 - accuracy: 0.9405 - val_loss: 0.2536 - val_accuracy: 0.9149\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.2630 - accuracy: 0.9080\n",
      "n_neurons: 200, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 41s 32ms/step - loss: 1.0839 - accuracy: 0.5959 - val_loss: 0.6376 - val_accuracy: 0.7587\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.6546 - accuracy: 0.7562 - val_loss: 0.5572 - val_accuracy: 0.7809\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.5567 - accuracy: 0.7934 - val_loss: 0.4579 - val_accuracy: 0.8291\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.4990 - accuracy: 0.8162 - val_loss: 0.4310 - val_accuracy: 0.8396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4599 - accuracy: 0.8324 - val_loss: 0.4010 - val_accuracy: 0.8523\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4300 - accuracy: 0.8426 - val_loss: 0.3786 - val_accuracy: 0.8613\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4084 - accuracy: 0.8523 - val_loss: 0.3560 - val_accuracy: 0.8685\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.3897 - accuracy: 0.8591 - val_loss: 0.3435 - val_accuracy: 0.8743\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.3711 - accuracy: 0.8653 - val_loss: 0.3288 - val_accuracy: 0.8807\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.3628 - accuracy: 0.8688 - val_loss: 0.3202 - val_accuracy: 0.8868\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.3247 - accuracy: 0.8817\n",
      "n_neurons: 200, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 40s 31ms/step - loss: 1.0956 - accuracy: 0.6057 - val_loss: 0.6220 - val_accuracy: 0.7724\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6458 - accuracy: 0.7580 - val_loss: 0.5726 - val_accuracy: 0.7765\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.5544 - accuracy: 0.7937 - val_loss: 0.4636 - val_accuracy: 0.8275\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4998 - accuracy: 0.8141 - val_loss: 0.4239 - val_accuracy: 0.8447\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.4601 - accuracy: 0.8321 - val_loss: 0.4062 - val_accuracy: 0.8516\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.4320 - accuracy: 0.8434 - val_loss: 0.3699 - val_accuracy: 0.8669\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 39s 32ms/step - loss: 0.4120 - accuracy: 0.8500 - val_loss: 0.3628 - val_accuracy: 0.8689\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.3945 - accuracy: 0.8574 - val_loss: 0.3418 - val_accuracy: 0.8717\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.3810 - accuracy: 0.8625 - val_loss: 0.3334 - val_accuracy: 0.8780\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.3661 - accuracy: 0.8669 - val_loss: 0.3248 - val_accuracy: 0.8827\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.3280 - accuracy: 0.8813\n",
      "n_neurons: 200, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 1.0892 - accuracy: 0.5950 - val_loss: 0.6694 - val_accuracy: 0.7509\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.6586 - accuracy: 0.7508 - val_loss: 0.5316 - val_accuracy: 0.7953\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.5649 - accuracy: 0.7885 - val_loss: 0.4674 - val_accuracy: 0.8351\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.5100 - accuracy: 0.8110 - val_loss: 0.4334 - val_accuracy: 0.8411\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.4700 - accuracy: 0.8278 - val_loss: 0.4110 - val_accuracy: 0.8452\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.4405 - accuracy: 0.8400 - val_loss: 0.3767 - val_accuracy: 0.8640\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.4156 - accuracy: 0.8494 - val_loss: 0.3665 - val_accuracy: 0.8671\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.3959 - accuracy: 0.8543 - val_loss: 0.3457 - val_accuracy: 0.8760\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.3800 - accuracy: 0.8621 - val_loss: 0.3411 - val_accuracy: 0.8752\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.3669 - accuracy: 0.8658 - val_loss: 0.3250 - val_accuracy: 0.8811\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3279 - accuracy: 0.8815\n",
      "n_neurons: 200, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 1.0820 - accuracy: 0.6053 - val_loss: 0.6626 - val_accuracy: 0.7598\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.6503 - accuracy: 0.7601 - val_loss: 0.5280 - val_accuracy: 0.8042\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.5521 - accuracy: 0.7963 - val_loss: 0.4499 - val_accuracy: 0.8391\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.4921 - accuracy: 0.8204 - val_loss: 0.4402 - val_accuracy: 0.8338\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.4562 - accuracy: 0.8318 - val_loss: 0.3927 - val_accuracy: 0.8557\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.4262 - accuracy: 0.8445 - val_loss: 0.3762 - val_accuracy: 0.8666\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.4038 - accuracy: 0.8528 - val_loss: 0.3704 - val_accuracy: 0.8652\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.3886 - accuracy: 0.8582 - val_loss: 0.3387 - val_accuracy: 0.8781\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.3701 - accuracy: 0.8654 - val_loss: 0.3399 - val_accuracy: 0.8755\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.3561 - accuracy: 0.8715 - val_loss: 0.3176 - val_accuracy: 0.8844\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3206 - accuracy: 0.8823\n",
      "n_neurons: 200, opt_type: sgd\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 1.0611 - accuracy: 0.6055 - val_loss: 0.6338 - val_accuracy: 0.7733\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.6330 - accuracy: 0.7622 - val_loss: 0.5092 - val_accuracy: 0.8059\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.5401 - accuracy: 0.8002 - val_loss: 0.4601 - val_accuracy: 0.8356\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.4864 - accuracy: 0.8216 - val_loss: 0.4137 - val_accuracy: 0.8512\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.4547 - accuracy: 0.8345 - val_loss: 0.3921 - val_accuracy: 0.8549\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.4254 - accuracy: 0.8452 - val_loss: 0.3779 - val_accuracy: 0.8682\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.4032 - accuracy: 0.8540 - val_loss: 0.3682 - val_accuracy: 0.8647\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.3888 - accuracy: 0.8594 - val_loss: 0.3491 - val_accuracy: 0.8745\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.3733 - accuracy: 0.8656 - val_loss: 0.3390 - val_accuracy: 0.8745\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.3588 - accuracy: 0.8709 - val_loss: 0.3280 - val_accuracy: 0.8810\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3442 - accuracy: 0.8756\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 44s 35ms/step - loss: 0.5205 - accuracy: 0.8101 - val_loss: 0.3172 - val_accuracy: 0.8851\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.3195 - accuracy: 0.8843 - val_loss: 0.2686 - val_accuracy: 0.9005\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.2683 - accuracy: 0.9013 - val_loss: 0.2457 - val_accuracy: 0.9094\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.2326 - accuracy: 0.9151 - val_loss: 0.2400 - val_accuracy: 0.9135\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.2020 - accuracy: 0.9253 - val_loss: 0.2286 - val_accuracy: 0.9199\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.1820 - accuracy: 0.9319 - val_loss: 0.2336 - val_accuracy: 0.9160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.1623 - accuracy: 0.9405 - val_loss: 0.2556 - val_accuracy: 0.9101\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.1433 - accuracy: 0.9466 - val_loss: 0.2323 - val_accuracy: 0.9217\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.1271 - accuracy: 0.9530 - val_loss: 0.2357 - val_accuracy: 0.9225\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 0.1168 - accuracy: 0.9577 - val_loss: 0.2627 - val_accuracy: 0.9211\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.2769 - accuracy: 0.9183\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.5163 - accuracy: 0.8109 - val_loss: 0.3209 - val_accuracy: 0.8820\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.3287 - accuracy: 0.8804 - val_loss: 0.2716 - val_accuracy: 0.8998\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.2757 - accuracy: 0.8981 - val_loss: 0.2600 - val_accuracy: 0.9030\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.2435 - accuracy: 0.9104 - val_loss: 0.2330 - val_accuracy: 0.9157\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.2133 - accuracy: 0.9208 - val_loss: 0.2369 - val_accuracy: 0.9118\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1892 - accuracy: 0.9295 - val_loss: 0.2251 - val_accuracy: 0.9206\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1716 - accuracy: 0.9366 - val_loss: 0.2385 - val_accuracy: 0.9157\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1514 - accuracy: 0.9438 - val_loss: 0.2464 - val_accuracy: 0.9180\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.1395 - accuracy: 0.9481 - val_loss: 0.2347 - val_accuracy: 0.9234\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.1199 - accuracy: 0.9552 - val_loss: 0.2529 - val_accuracy: 0.9220\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.2582 - accuracy: 0.9200\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.5217 - accuracy: 0.8089 - val_loss: 0.3208 - val_accuracy: 0.8831\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.3234 - accuracy: 0.8839 - val_loss: 0.2692 - val_accuracy: 0.9010\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.2681 - accuracy: 0.9021 - val_loss: 0.2493 - val_accuracy: 0.9073\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.2368 - accuracy: 0.9132 - val_loss: 0.2432 - val_accuracy: 0.9082\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.2061 - accuracy: 0.9240 - val_loss: 0.2454 - val_accuracy: 0.9132\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.1833 - accuracy: 0.9311 - val_loss: 0.2241 - val_accuracy: 0.9183\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.1637 - accuracy: 0.9378 - val_loss: 0.2300 - val_accuracy: 0.9175\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.1464 - accuracy: 0.9463 - val_loss: 0.2269 - val_accuracy: 0.9238\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.1331 - accuracy: 0.9517 - val_loss: 0.2538 - val_accuracy: 0.9173\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 0.1215 - accuracy: 0.9538 - val_loss: 0.2422 - val_accuracy: 0.9247\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.2528 - accuracy: 0.9223\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.5053 - accuracy: 0.8148 - val_loss: 0.3130 - val_accuracy: 0.8885\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.3164 - accuracy: 0.8851 - val_loss: 0.2554 - val_accuracy: 0.9097\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.2627 - accuracy: 0.9047 - val_loss: 0.2419 - val_accuracy: 0.9145\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.2325 - accuracy: 0.9140 - val_loss: 0.2590 - val_accuracy: 0.9066\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.2042 - accuracy: 0.9245 - val_loss: 0.2238 - val_accuracy: 0.9238\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.1806 - accuracy: 0.9327 - val_loss: 0.2237 - val_accuracy: 0.9216\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.1622 - accuracy: 0.9399 - val_loss: 0.2332 - val_accuracy: 0.9209\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.1426 - accuracy: 0.9470 - val_loss: 0.2466 - val_accuracy: 0.9177\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.1256 - accuracy: 0.9531 - val_loss: 0.2499 - val_accuracy: 0.9266\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 43s 35ms/step - loss: 0.1150 - accuracy: 0.9583 - val_loss: 0.2542 - val_accuracy: 0.9232\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.2698 - accuracy: 0.9193\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 44s 35ms/step - loss: 0.5173 - accuracy: 0.8108 - val_loss: 0.3415 - val_accuracy: 0.8722\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.3243 - accuracy: 0.8827 - val_loss: 0.2836 - val_accuracy: 0.8952\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.2735 - accuracy: 0.9012 - val_loss: 0.2538 - val_accuracy: 0.9068\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.2417 - accuracy: 0.9103 - val_loss: 0.2370 - val_accuracy: 0.9126\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.2147 - accuracy: 0.9205 - val_loss: 0.2273 - val_accuracy: 0.9188\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.1904 - accuracy: 0.9302 - val_loss: 0.2189 - val_accuracy: 0.9177\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 43s 34ms/step - loss: 0.1691 - accuracy: 0.9376 - val_loss: 0.2399 - val_accuracy: 0.9175\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.1511 - accuracy: 0.9443 - val_loss: 0.2465 - val_accuracy: 0.9172\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.1391 - accuracy: 0.9494 - val_loss: 0.2400 - val_accuracy: 0.9207\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 43s 35ms/step - loss: 0.1270 - accuracy: 0.9536 - val_loss: 0.2522 - val_accuracy: 0.9203\n",
      "313/313 [==============================] - 4s 9ms/step - loss: 0.2701 - accuracy: 0.9143\n",
      "n_neurons: 200, opt_type: adam\n",
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 57s 36ms/step - loss: 0.4845 - accuracy: 0.8228 - val_loss: 0.3060 - val_accuracy: 0.8925\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.3045 - accuracy: 0.8892 - val_loss: 0.2532 - val_accuracy: 0.9064\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.2562 - accuracy: 0.9068 - val_loss: 0.2543 - val_accuracy: 0.9065\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.2250 - accuracy: 0.9163 - val_loss: 0.2322 - val_accuracy: 0.9159\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.2004 - accuracy: 0.9262 - val_loss: 0.2138 - val_accuracy: 0.9234\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.1787 - accuracy: 0.9360 - val_loss: 0.2146 - val_accuracy: 0.9217\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.1614 - accuracy: 0.9404 - val_loss: 0.2249 - val_accuracy: 0.9208\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 52s 34ms/step - loss: 0.1451 - accuracy: 0.9457 - val_loss: 0.2246 - val_accuracy: 0.9232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.1320 - accuracy: 0.9515 - val_loss: 0.2550 - val_accuracy: 0.9133\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.1196 - accuracy: 0.9552 - val_loss: 0.2338 - val_accuracy: 0.9256\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=&lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x7fcf9a55dc10&gt;,\n",
       "             param_grid={&#x27;n_neurons&#x27;: [100, 200], &#x27;opt_type&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=&lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x7fcf9a55dc10&gt;,\n",
       "             param_grid={&#x27;n_neurons&#x27;: [100, 200], &#x27;opt_type&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x7fcf9a55dc10&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x7fcf9a55dc10&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7fcf9a55dc10>,\n",
       "             param_grid={'n_neurons': [100, 200], 'opt_type': ['sgd', 'adam']})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(keras_classifier, parameters, cv=5)\n",
    "\n",
    "grid_search.fit(X_train, \n",
    "                y_train, \n",
    "                epochs=10, \n",
    "                validation_data = (X_valid, y_valid), \n",
    "                callbacks=EarlyStopping(patience=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffe563",
   "metadata": {},
   "source": [
    "Having finished the hyperparameter fine tuning, the best parameters determined are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22ae5783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neurons': 200, 'opt_type': 'adam'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3996e64",
   "metadata": {},
   "source": [
    "These parameters have achieved the following validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e5bea69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9188400030136108"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a2be8c",
   "metadata": {},
   "source": [
    "## Optimial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e395b31",
   "metadata": {},
   "source": [
    "Using this optimal model, we finally assess the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc5eff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = grid_search.best_estimator_.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a51dfad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_42 (Conv2D)          (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_42 (MaxPoolin  (None, 14, 14, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_43 (Conv2D)          (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_43 (MaxPoolin  (None, 7, 7, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_21 (Flatten)        (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 200)               627400    \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 688,426\n",
      "Trainable params: 688,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimal_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40dba60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.2536 - accuracy: 0.9206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25356125831604004, 0.9205999970436096]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38ccee",
   "metadata": {},
   "source": [
    "As we can see, this has achieved a slightly better performance on the test dataset. Overall, we achieved very strong classification performance on the MNIST Fashion Dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
